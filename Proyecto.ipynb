{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python -m pip install pymupdf pandas openpyxl tensorflow tensorflow_hub tensorflow-text spacy classy-classification "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python -m spacy download es_dep_news_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwxgY6M8rIDS"
   },
   "source": [
    "# Resumen Ejecutivo\n",
    "Durante el proceso de revisión de los informes de práctica del DISC (Departamento de Ingeniería de Sistemas y Computación), se requiere una inversión considerable de tiempo que, hasta la fecha, no ha sido automatizada. Esto conlleva largas jornadas de trabajo y carga adicional para los académicos, quienes podrían emplear ese tiempo en otras labores. Por lo tanto, como equipo de trabajo, hemos llegado a un consenso en la necesidad de llevar a cabo el análisis y desarrollo de un modelo que permita clasificar los informes en las categorías definidas en la rúbrica actual (insatisfactorio, regular, bueno y excelente).\n",
    "Es importante destacar que, con la llegada de la pandemia, la entrega de informes ha sido en formato digital, lo que ha generado un conjunto de aproximadamente 100 informes disponibles. Esta digitalización ofrece ventajas significativas para el entrenamiento del modelo, ya que se dispone de datos de entrada y resultados concretos (informe, rúbrica y nota).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T02:19:36.643268100Z",
     "start_time": "2023-11-26T02:19:35.080739100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "error",
     "timestamp": 1698963780743,
     "user": {
      "displayName": "DAVID NAHUM ARAYA CADIZ",
      "userId": "08643668274056945811"
     },
     "user_tz": 180
    },
    "id": "50OQQBE7rlzE",
    "outputId": "a1ca34b1-97f5-4af3-affd-d781bb3f766b"
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-10T23:40:00.959926900Z",
     "start_time": "2023-10-10T23:40:00.944208Z"
    },
    "id": "GBIB6u4nrnD-"
   },
   "source": [
    "# Lectura de datos\n",
    "Se cargan la información del dataset, y se eliminan los datos nulos de las calificaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T02:19:38.586138500Z",
     "start_time": "2023-11-26T02:19:36.589432300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "error",
     "timestamp": 1698959147784,
     "user": {
      "displayName": "DAVID NAHUM ARAYA CADIZ",
      "userId": "08643668274056945811"
     },
     "user_tz": 180
    },
    "id": "ALsO6MCgzB3W",
    "outputId": "93af515f-9b20-4e0f-c860-eeb9dae4dc71"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(\"calificaciones.xlsx\", decimal=',')\n",
    "grades_columns = dataset.columns.difference([\"id\", \"periodo\", \"Unnamed: 9\"]) #[\"estructura\", \"escritura\", \"contenido\", \"conclusiones\", \"conocimiento\", \"relevancia\", \"total\"]\n",
    "rubric_columns = grades_columns.difference([\"total\"]) #, \"escritura\", \"estructura\"\n",
    "dataset = dataset.dropna(subset=grades_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHe-8AbczB3X"
   },
   "source": [
    "# Extracción y limpieza de documentos\n",
    "En esta sección, se cargan los documentos en formato PDF, para la extracción y limpieza de estos, seguido de su integración al dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T02:20:08.715233500Z",
     "start_time": "2023-11-26T02:19:38.649681500Z"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1696970321635,
     "user": {
      "displayName": "JOSE MANUEL ALCAYAGA MARIN",
      "userId": "01460857505703722093"
     },
     "user_tz": 180
    },
    "id": "5oOHwTYSry49",
    "outputId": "4a3d3351-aef6-452e-ddc5-034301bc4eea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>periodo</th>\n",
       "      <th>documents</th>\n",
       "      <th>estructura</th>\n",
       "      <th>escritura</th>\n",
       "      <th>contenido</th>\n",
       "      <th>conclusiones</th>\n",
       "      <th>conocimiento</th>\n",
       "      <th>relevancia</th>\n",
       "      <th>total</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20908397-1</td>\n",
       "      <td>2023-1</td>\n",
       "      <td>\\n  \\nUNIVERSIDAD CATÓLICA DEL NORTE  \\nFACUL...</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18971994-1</td>\n",
       "      <td>2023-1</td>\n",
       "      <td>\\nAntofagasta \\n \\n           Abril de 2023 \\...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19445943-1</td>\n",
       "      <td>2023-1</td>\n",
       "      <td>\\n1 \\n \\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\n...</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.9</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19463712-1</td>\n",
       "      <td>2023-1</td>\n",
       "      <td>\\n \\n \\n \\n                                  ...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20218430-1</td>\n",
       "      <td>2023-1</td>\n",
       "      <td>\\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>19928371-1</td>\n",
       "      <td>2021-1</td>\n",
       "      <td>\\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>19952605-1</td>\n",
       "      <td>2021-1</td>\n",
       "      <td>\\n \\n \\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nF...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>19957163-1</td>\n",
       "      <td>2021-1</td>\n",
       "      <td>\\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTA...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>20180533-1</td>\n",
       "      <td>2021-1</td>\n",
       "      <td>\\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>20408302-1</td>\n",
       "      <td>2021-1</td>\n",
       "      <td>\\ni \\n \\nx \\n \\nUNIVERSIDAD CATÓLICA DEL NORT...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id periodo                                          documents  \\\n",
       "0    20908397-1  2023-1   \\n  \\nUNIVERSIDAD CATÓLICA DEL NORTE  \\nFACUL...   \n",
       "1    18971994-1  2023-1   \\nAntofagasta \\n \\n           Abril de 2023 \\...   \n",
       "2    19445943-1  2023-1   \\n1 \\n \\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\n...   \n",
       "5    19463712-1  2023-1   \\n \\n \\n \\n                                  ...   \n",
       "6    20218430-1  2023-1   \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...   \n",
       "..          ...     ...                                                ...   \n",
       "177  19928371-1  2021-1   \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...   \n",
       "178  19952605-1  2021-1   \\n \\n \\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nF...   \n",
       "179  19957163-1  2021-1   \\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTA...   \n",
       "180  20180533-1  2021-1   \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...   \n",
       "181  20408302-1  2021-1   \\ni \\n \\nx \\n \\nUNIVERSIDAD CATÓLICA DEL NORT...   \n",
       "\n",
       "     estructura  escritura  contenido  conclusiones  conocimiento  relevancia  \\\n",
       "0           6.2        5.1        6.0           5.5           4.4         6.0   \n",
       "1           6.9        6.8        6.8           6.8           7.0         6.9   \n",
       "2           6.7        6.9        6.5           6.4           6.8         7.0   \n",
       "5           4.4        4.9        4.9           5.8           4.0         6.0   \n",
       "6           6.1        5.8        5.5           5.0           4.5         5.8   \n",
       "..          ...        ...        ...           ...           ...         ...   \n",
       "177         6.8        6.2        6.0           5.7           6.3         6.0   \n",
       "178         7.0        6.8        6.8           7.0           7.0         7.0   \n",
       "179         6.5        4.5        5.8           5.5           6.4         6.3   \n",
       "180         7.0        6.0        6.7           6.8           6.8         7.0   \n",
       "181         6.5        5.0        6.0           6.5           6.5         6.5   \n",
       "\n",
       "     total Unnamed: 9  \n",
       "0      5.3        NaN  \n",
       "1      6.9        NaN  \n",
       "2      6.7        NaN  \n",
       "5      4.9        NaN  \n",
       "6      5.2        NaN  \n",
       "..     ...        ...  \n",
       "177    6.1        NaN  \n",
       "178    6.9        NaN  \n",
       "179    5.8        NaN  \n",
       "180    6.7        NaN  \n",
       "181    6.2        NaN  \n",
       "\n",
       "[174 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for id in dataset['id']:\n",
    "    pdf_file = fitz.open(f\"dataset/{id}.pdf\")\n",
    "    document_text = chr(12).join([page.get_text() for page in pdf_file])\n",
    "    documents.append(document_text)\n",
    "\n",
    "dataset.insert(loc=2, column=\"documents\", value=documents)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Q4tHjIzzB3Y"
   },
   "source": [
    "# Preprocesamiento de etiquetas\n",
    "Con base en las notas obtenidas por cada entrada, se clasifican los documentos en las categorías definidas en la rúbrica.\n",
    "Esto es, se reemplaza la nota de cada componente de cada informe por un elemento que represente la categoría correspondiente, el cual puede ser un texto que sea directamente el nombre de la categoría, o un número del 0 para \"insuficiente\" hasta el 3 para \"excelente\".\n",
    "El primero se utiliza para específicamente el modelo Classy Classification, mientras que el segundo se utiliza para el resto de modelos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T02:19:36.671363800Z",
     "start_time": "2023-11-26T02:19:36.589432300Z"
    },
    "id": "BKmoSmVHzB3W"
   },
   "outputs": [],
   "source": [
    "def get_classification(grade, number=False):\n",
    "  grade = round(grade, 1)\n",
    "  if(grade < 4):\n",
    "    return \"insatisfactorio\" if not number else 0\n",
    "  elif (4 <= grade < 5.5):\n",
    "    return \"regular\" if not number else 1\n",
    "  elif (5.5 <= grade < 6.5):\n",
    "    return \"bueno\" if not number else 2\n",
    "  elif (6.5 <= grade <= 7):\n",
    "    return \"excelente\" if not number else 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T02:20:08.716495200Z",
     "start_time": "2023-11-26T02:20:08.662301700Z"
    },
    "id": "mleH6J-3zB3Y",
    "outputId": "20495e2d-9827-4989-c04d-c8cb67b5429b"
   },
   "outputs": [],
   "source": [
    "text_labeled_dataset = dataset.copy()\n",
    "text_labeled_dataset.loc[:, grades_columns] = text_labeled_dataset.loc[:, grades_columns].apply(lambda s: s.apply(get_classification))\n",
    "dataset.loc[:, grades_columns] = dataset.loc[:, grades_columns].apply(lambda s: s.apply(lambda x: get_classification(grade=x, number=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYMJnSAdzB3Z"
   },
   "source": [
    "# Clasificación de documentos con métodos tradicionales\n",
    "Se usará SciKit-Learn para el análisis de los documentos mediante métodos tradicionales de NLP, específicamente TF-IDF. Se utilizarán distintos modelos de Machine Learning para la clasificación de los documentos, como regresión lineal y logística, SVM, árboles de decisión y Naïve Bayes.\n",
    "En primera instancia, solo se probará la clasificiación final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T02:20:22.513360500Z",
     "start_time": "2023-11-26T02:20:08.708074400Z"
    },
    "id": "1lNHqqk-zB3Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, CategoricalNB\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from numpy import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento TF-IDF\n",
    "Para el trabajo de los modelos tradicionales, se usa representación TF-IDF. Se aplica la división en conjunto de prueba y validación, y se aplica la vectorización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T02:20:22.574858300Z",
     "start_time": "2023-11-26T02:20:22.574858300Z"
    },
    "id": "BHDzcXQXzB3Z"
   },
   "outputs": [],
   "source": [
    "Xn = dataset['documents']\n",
    "yn = dataset[grades_columns]\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xn, yn, random_state=3, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T13:31:56.686125200Z",
     "start_time": "2023-11-13T13:31:53.586042500Z"
    },
    "id": "z0HQfhUBzB3a"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.9, min_df=0.2)\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresores de nota final\n",
    "A modo de experimentación, para obtener un modelo que pueda predecir una nota final considerando una predicción de todos los elementos, se intentan hacer modelos que puedan definir la categoría final en base a los elementos de la rúbrica. Además, se prueba aplicando la fórmula de nota final considerando solamente las categorías de la rúbrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T14:08:35.651595500Z",
     "start_time": "2023-11-13T14:08:35.634519600Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_final_grade(rubric_grades):\n",
    "    result = rubric_grades['estructura']*0.5\n",
    "    result += rubric_grades['escritura']*0.15\n",
    "    result += rubric_grades['contenido']*0.25\n",
    "    result += rubric_grades['conclusiones']*0.15\n",
    "    result += rubric_grades['conocimiento']*0.30\n",
    "    result += rubric_grades['relevancia']*0.10\n",
    "    result \n",
    "    return floor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T14:11:21.995464600Z",
     "start_time": "2023-11-13T14:11:21.919605700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3390804597701149\n"
     ]
    }
   ],
   "source": [
    "y_calc = get_final_grade(yn[rubric_columns])\n",
    "print(accuracy_score(y_calc, yn['total']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T14:01:40.439103800Z",
     "start_time": "2023-11-13T14:01:40.314202500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.8113207547169812, F1: 0.8131925726265348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Característica': Index(['conclusiones', 'conocimiento', 'contenido', 'escritura', 'estructura',\n",
       "        'relevancia'],\n",
       "       dtype='object'),\n",
       " 'Importancia': array([0.17987443, 0.2466199 , 0.24701639, 0.20052356, 0.06050739,\n",
       "        0.06545833])}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_grade_rf = RandomForestClassifier()\n",
    "final_grade_rf.fit(y_train[rubric_columns], y_train['total'])\n",
    "grade_pred_rf = final_grade_rf.predict(y_test[rubric_columns])\n",
    "acc = accuracy_score(y_test['total'], grade_pred_rf)\n",
    "f1 = f1_score(y_test['total'], grade_pred_rf, average='weighted')\n",
    "print(f'Accuraccy: {acc}, F1: {f1}')\n",
    "{'Característica': rubric_columns, 'Importancia': final_grade_rf.feature_importances_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.8301886792452831, F1: 0.8401467505241089\n"
     ]
    }
   ],
   "source": [
    "final_grade_svc_n = SVC(C=10)\n",
    "final_grade_svc_n.fit(y_train[rubric_columns], y_train['total'])\n",
    "grade_pred = final_grade_svc_n.predict(y_test[rubric_columns])\n",
    "acc = accuracy_score(y_test['total'], grade_pred)\n",
    "f1 = f1_score(y_test['total'], grade_pred, average='weighted')\n",
    "print(f'Accuraccy: {acc}, F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T14:01:41.649059400Z",
     "start_time": "2023-11-13T14:01:41.602514400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.8679245283018868, F1: 0.8687345149609299\n"
     ]
    }
   ],
   "source": [
    "final_grade_log = LogisticRegression(max_iter=1000)\n",
    "final_grade_log.fit(y_train[rubric_columns], y_train['total'])\n",
    "grade_pred_log = final_grade_log.predict(y_test[rubric_columns])\n",
    "acc = accuracy_score(y_test['total'], grade_pred_log)\n",
    "f1 = f1_score(y_test['total'], grade_pred_log, average='weighted')\n",
    "print(f'Accuraccy: {acc}, F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:18:54.597476100Z",
     "start_time": "2023-11-13T15:18:54.546424800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.8490566037735849, F1: 0.8503294399520814\n"
     ]
    }
   ],
   "source": [
    "final_grade_nb = CategoricalNB()\n",
    "final_grade_nb.fit(y_train[rubric_columns], y_train['total'])\n",
    "grade_pred_nb = final_grade_nb.predict(y_test[rubric_columns])\n",
    "acc = accuracy_score(y_test['total'], grade_pred_nb)\n",
    "f1 = f1_score(y_test['total'], grade_pred_nb, average='weighted')\n",
    "print(f'Accuraccy: {acc}, F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.8113207547169812, F1: 0.811572327044025\n"
     ]
    }
   ],
   "source": [
    "final_grade_xgb = XGBClassifier()\n",
    "final_grade_xgb.fit(y_train[rubric_columns], y_train['total'])\n",
    "grade_pred_xgb = final_grade_xgb.predict(y_test[rubric_columns])\n",
    "acc = accuracy_score(y_test['total'], grade_pred_xgb)\n",
    "f1 = f1_score(y_test['total'], grade_pred_xgb, average='weighted')\n",
    "print(f'Accuraccy: {acc}, F1: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de documentos con SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T14:39:07.734194600Z",
     "start_time": "2023-11-13T14:39:07.421751300Z"
    },
    "id": "rs1HVwG_zB3a",
    "outputId": "38650c0d-f462-4168-966f-5f0b0dca9819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.5849056603773585, F1: 0.5516265369466873\n"
     ]
    }
   ],
   "source": [
    "svc_n = SVC(C=10)\n",
    "svc_n.fit(X_train_bow, y_train['total'])\n",
    "y_pred = svc_n.predict(X_test_bow)\n",
    "acc = accuracy_score(y_test['total'], y_pred)\n",
    "f1 = f1_score(y_test['total'], y_pred, average='weighted')\n",
    "print(f'Accuraccy: {acc}, F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T14:37:59.441941100Z",
     "start_time": "2023-11-13T14:37:56.923915Z"
    },
    "id": "yqKqRUmNzB3a",
    "outputId": "fbae29c6-e7fd-4556-9f6c-37ed6270250a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.07547169811320754\n"
     ]
    }
   ],
   "source": [
    "svc_n_mo = MultiOutputClassifier(svc_n)\n",
    "svc_n_mo.fit(X_train_bow, y_train[rubric_columns])\n",
    "y_pred_mo = svc_n_mo.predict(X_test_bow)\n",
    "acc_mo = svc_n_mo.score(X_test_bow, y_test[rubric_columns])\n",
    "print(f'Accuracy: {acc_mo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de documentos con Regresión Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T14:39:01.699203700Z",
     "start_time": "2023-11-13T14:39:01.628719100Z"
    },
    "id": "WTDlUHWwzB3a",
    "outputId": "fa85e054-e909-4275-c010-e2f1b5b3e067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.5660377358490566, F1: 0.5275254892769249\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "ridge.fit(X_train_bow, y_train['total'])\n",
    "y_pred_r = ridge.predict(X_test_bow)\n",
    "acc_r = accuracy_score(y_test['total'], y_pred_r)\n",
    "f1_r = f1_score(y_test['total'], y_pred_r, average='weighted')\n",
    "print(f'Accuraccy: {acc_r}, F1: {f1_r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T13:46:56.828983100Z",
     "start_time": "2023-11-13T13:46:56.698586700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.03773584905660377\n"
     ]
    }
   ],
   "source": [
    "ridge_mo = MultiOutputClassifier(ridge)\n",
    "ridge_mo.fit(X_train_bow, y_train[rubric_columns])\n",
    "y_pred_r_mo = ridge_mo.predict(X_test_bow)\n",
    "acc_r_mo = ridge_mo.score(X_test_bow, y_test[rubric_columns])\n",
    "print(f'Accuracy: {acc_r_mo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de documentos con Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T14:38:53.331610400Z",
     "start_time": "2023-11-13T14:38:53.215161700Z"
    },
    "id": "5hAxzAUJzB3a",
    "outputId": "1871a340-3e03-4033-95d2-2259223f59a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.6037735849056604, F1: 0.5322023148882195\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(X_train_bow, y_train['total'])\n",
    "y_pred_l = log.predict(X_test_bow)\n",
    "acc_l = accuracy_score(y_test['total'], y_pred_l)\n",
    "f1_l = f1_score(y_test['total'], y_pred_l, average='weighted')\n",
    "print(f'Accuraccy: {acc_l}, F1: {f1_l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de documentos con Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T13:31:58.787004700Z",
     "start_time": "2023-11-13T13:31:58.111578400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.5471698113207547, F1: 0.4873391291143085\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_bow, y_train['total'])\n",
    "y_pred_rf = rf.predict(X_test_bow)\n",
    "acc_rf = accuracy_score(y_test['total'], y_pred_rf)\n",
    "f1_rf = f1_score(y_test['total'], y_pred_rf, average='weighted')\n",
    "print(f'Accuraccy: {acc_rf}, F1: {f1_rf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T14:25:11.122720300Z",
     "start_time": "2023-11-13T14:25:07.570772600Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 2, 2, 3],\n",
       "       [3, 3, 2, 1, 3, 3],\n",
       "       [3, 2, 2, 2, 3, 3],\n",
       "       [3, 1, 2, 1, 3, 2],\n",
       "       [3, 3, 2, 1, 3, 3],\n",
       "       [3, 3, 2, 2, 3, 2],\n",
       "       [3, 3, 2, 1, 3, 3],\n",
       "       [2, 1, 2, 2, 3, 2],\n",
       "       [3, 3, 2, 2, 3, 3],\n",
       "       [3, 2, 2, 2, 3, 3],\n",
       "       [3, 2, 2, 1, 3, 3],\n",
       "       [3, 3, 2, 2, 3, 3],\n",
       "       [3, 3, 2, 2, 3, 3],\n",
       "       [3, 2, 2, 2, 3, 3],\n",
       "       [2, 2, 2, 2, 2, 3],\n",
       "       [3, 1, 3, 1, 3, 3],\n",
       "       [2, 3, 2, 2, 3, 3],\n",
       "       [3, 3, 2, 1, 3, 3],\n",
       "       [3, 3, 2, 2, 3, 3],\n",
       "       [3, 1, 2, 1, 3, 3],\n",
       "       [2, 1, 2, 1, 2, 2],\n",
       "       [3, 1, 2, 1, 3, 3],\n",
       "       [3, 2, 2, 2, 3, 3],\n",
       "       [3, 3, 2, 2, 3, 2],\n",
       "       [2, 1, 2, 1, 2, 2],\n",
       "       [3, 3, 3, 1, 3, 3],\n",
       "       [2, 1, 2, 1, 2, 3],\n",
       "       [3, 3, 2, 2, 3, 2],\n",
       "       [3, 3, 2, 1, 3, 3],\n",
       "       [3, 3, 2, 2, 3, 3],\n",
       "       [2, 1, 2, 1, 3, 3],\n",
       "       [3, 3, 2, 2, 3, 3],\n",
       "       [3, 3, 2, 2, 3, 3],\n",
       "       [3, 3, 2, 1, 3, 3],\n",
       "       [2, 2, 2, 1, 3, 2],\n",
       "       [3, 1, 2, 2, 3, 3],\n",
       "       [3, 3, 2, 2, 3, 3],\n",
       "       [3, 2, 2, 1, 3, 3],\n",
       "       [3, 2, 2, 2, 3, 3],\n",
       "       [3, 2, 2, 1, 3, 3],\n",
       "       [3, 3, 2, 2, 3, 3],\n",
       "       [3, 3, 2, 1, 3, 3],\n",
       "       [3, 2, 2, 1, 3, 3],\n",
       "       [2, 2, 2, 2, 2, 3],\n",
       "       [3, 3, 2, 2, 2, 2],\n",
       "       [2, 1, 2, 1, 3, 3],\n",
       "       [3, 3, 2, 2, 2, 2],\n",
       "       [3, 1, 2, 1, 2, 3],\n",
       "       [3, 1, 2, 1, 2, 3],\n",
       "       [3, 3, 2, 2, 3, 3],\n",
       "       [3, 1, 2, 1, 3, 3],\n",
       "       [3, 3, 2, 2, 3, 3],\n",
       "       [2, 3, 1, 1, 2, 2]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mo = MultiOutputClassifier(rf)\n",
    "rf_mo.fit(X_train_bow, y_train[rubric_columns])\n",
    "y_pred_rf_mo = rf_mo.predict(X_test_bow)\n",
    "acc_rf_mo = rf_mo.score(X_test_bow, y_test[rubric_columns])\n",
    "print(f'Accuracy: {acc_rf_mo}')\n",
    "\n",
    "y_pred_rf_mo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de documentos con Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T13:33:42.643289600Z",
     "start_time": "2023-11-13T13:33:42.571578500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.5660377358490566, F1: 0.41417395306028526\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_bow, y_train['total'])\n",
    "y_pred_nb = nb.predict(X_test_bow)\n",
    "acc_nb = accuracy_score(y_test['total'], y_pred_nb)\n",
    "f1_nb = f1_score(y_test['total'], y_pred_nb, average='weighted')\n",
    "print(f'Accuraccy: {acc_nb}, F1: {f1_nb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de documentos con XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraccy: 0.4339622641509434, F1: 0.41675727534378\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_bow, y_train['total'])\n",
    "y_pred_xgb = xgb.predict(X_test_bow)\n",
    "acc_xgb = accuracy_score(y_test['total'], y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test['total'], y_pred_xgb, average='weighted')\n",
    "print(f'Accuraccy: {acc_xgb}, F1: {f1_xgb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoRgmlrbzB3b"
   },
   "source": [
    "# Clasificación de documentos con Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de documentos con Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-13T13:29:54.204446200Z"
    },
    "id": "HMvJNW6rzB3b"
   },
   "outputs": [],
   "source": [
    "Xn_text_labeled = text_labeled_dataset['documents']\n",
    "yn_text_labeled = text_labeled_dataset['total']\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(Xn_text_labeled, yn_text_labeled, random_state=3, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-13T13:29:54.206970Z"
    },
    "id": "22OHNVvlzB3b",
    "outputId": "d7606e11-05f2-4b6f-a4ff-c6c3a258c23a"
   },
   "outputs": [],
   "source": [
    "training_data_total = {\n",
    "  \"insatisfactorio\": [],\n",
    "  \"regular\": [],\n",
    "  \"bueno\": [],\n",
    "  \"excelente\": []\n",
    "}\n",
    "for index, document in X_text_train.items():\n",
    "  training_data_total[y_text_train[index]].append(document)\n",
    "training_data_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOG-iZUXzB3b"
   },
   "source": [
    "https://spacy.io/universe/project/classyclassification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "mlVcxih6zB3b"
   },
   "source": [
    "import spacy\n",
    "from spacy_transformers import Transformer\n",
    "from spacy_transformers.pipeline_component import DEFAULT_CONFIG\n",
    "nlp = spacy.load(\"es_dep_news_trf\")\n",
    "nlp.add_pipe(\"classy_classification\",\n",
    "    config={\n",
    "        \"data\": training_data_total,\n",
    "        \"model\": \"spacy\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-13T13:29:54.208980500Z"
    },
    "id": "26ki2BbFzB3e"
   },
   "outputs": [],
   "source": [
    "from classy_classification import ClassyClassifier\n",
    "classifier = ClassyClassifier(data=training_data_total)\n",
    "classifier.set_embedding_model(model=\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "y_text_pred = classifier.pipe(X_text_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-13T13:29:54.213068700Z"
    },
    "id": "aGs2kN8XzB3f",
    "outputId": "764f471b-2fa4-473a-9019-7aa1aecc2cc7"
   },
   "outputs": [],
   "source": [
    "y_text_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-13T13:29:54.216093800Z"
    },
    "id": "Lr7IDg0SzB3f",
    "outputId": "621c6e2a-ccd6-49d9-fadc-6c175e9eedd4"
   },
   "outputs": [],
   "source": [
    "y_text_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de documentos con TensorFlow\n",
    "Se prueba un modelo LSTM para hacer la clasificación en base a una representación vectorial del cuerpo del documento.\n",
    "Por limitaciones técnicas y para evitar pérdida de información, cada documento se divide en un conjunto de bloques; por el momento se considera solamente la división propia del formato PDF.\n",
    "Así, la entrada del modelo LSTM sería un conjunto de lotes o muestras, que contemplan un conjunto de bloques, que a su vez son un vector de cierta dimensionalidad (determinada por el modelo de representación utilizado).\n",
    "Dado que el modelo LSTM requiere de entradas de tamaño fijo, se asignó un tamaño de entrada tal que todos los documentos pudieran ser transformados y aceptados por el modelo; para suplir el espacio restante de cada documento, se rellena la entrada con una representación de una cadena vacía.\n",
    "\n",
    "La búsqueda de modelos de representación se hizo priorizando el largo de entrada, y la posibilidad de trabajar con textos en español. Para acotar el trabajo a realizar, se escogieron 3 modelos a probar:\n",
    "- Universal Sentence Encoder - Multilingual Large: Tiene soporte para español y admite una entrada de tamaño arbitrario (a coste de posible pérdida de información).\n",
    "- Longformer Spanish: Modelo basado en BERT, mejorado para soportar entradas de hasta 4096 tokens,y entrenado específicamente en español.\n",
    "- Tulio BERT: Modelo basado en BERT, entrenado con un conjunto de datos chileno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T02:20:36.230073600Z",
     "start_time": "2023-11-26T02:20:22.574858300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4438\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "max_length = -1\n",
    "for id in dataset['id']:\n",
    "    pdf_file = fitz.open(f\"dataset/{id}.pdf\")\n",
    "    document_text = [] \n",
    "    for page in pdf_file:\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        document_text += [block[4] for block in blocks]\n",
    "    documents.append(document_text)\n",
    "    for block in document_text:\n",
    "        if len(block) > max_length:\n",
    "            max_length = len(block)\n",
    "print(max_length)            \n",
    "dataset.insert(loc=2, column=\"block_documents\", value=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de párafos corresponde al mayor número de bloques encontrado en un documento, aproximado a la siguiente mayor potencia de dos.\n",
    "El tamaño de lote corresponde a la cantidad de documentos que se procesaran.\n",
    "Además, para facilitar el trabajo de clasificación por parte del modelo, se transforman las categorías a una lista de represntación binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T02:21:36.290176800Z",
     "start_time": "2023-11-26T02:20:36.170241800Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
    "from keras.models import Sequential, clone_model\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "from keras import utils\n",
    "import torch\n",
    "PARAGRAPH_QTY = 2048\n",
    "BATCH_SIZE= 171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[\"estructura\", \"escritura\", \"contenido\", \"conclusiones\", \"conocimiento\", \"relevancia\", \"total\"]\n",
    "labels_total = utils.to_categorical(dataset['total'], num_classes=4)\n",
    "labels_estructura = utils.to_categorical(dataset['estructura'], num_classes=4)\n",
    "labels_escritura = utils.to_categorical(dataset['escritura'], num_classes=4)\n",
    "labels_contenido = utils.to_categorical(dataset['contenido'], num_classes=4)\n",
    "labels_conclusiones = utils.to_categorical(dataset['conclusiones'], num_classes=4)\n",
    "labels_conocimiento = utils.to_categorical(dataset['conocimiento'], num_classes=4)\n",
    "labels_relevancia = utils.to_categorical(dataset['relevancia'], num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación de documentos con Universal Sentence Encoder - Multilingual Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T17:57:43.889612700Z",
     "start_time": "2023-11-25T17:55:31.047558Z"
    },
    "id": "e51hCbTczB3f"
   },
   "outputs": [],
   "source": [
    "use_embedder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T19:10:34.967234Z",
     "start_time": "2023-11-25T19:10:34.783346900Z"
    }
   },
   "outputs": [],
   "source": [
    "use_pad = use_embedder(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T18:43:25.953487600Z",
     "start_time": "2023-11-25T18:06:31.571382100Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 11:50:49.636775: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1776424960 exceeds 10% of free system memory.\n",
      "2023-11-26 11:50:49.645256: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1776424960 exceeds 10% of free system memory.\n",
      "2023-11-26 11:50:49.705936: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1776424960 exceeds 10% of free system memory.\n",
      "2023-11-26 11:50:50.099502: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1776424960 exceeds 10% of free system memory.\n",
      "2023-11-26 11:50:50.181956: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1776424960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1619.3680374929681\n"
     ]
    }
   ],
   "source": [
    "Xn_text_embed = [use_embedder(document) for document in dataset['block_documents']]\n",
    "print(time_end-time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T19:12:22.672771800Z",
     "start_time": "2023-11-25T19:12:19.952407200Z"
    }
   },
   "outputs": [],
   "source": [
    "padded_documents = []\n",
    "for document in Xn_text_embed:\n",
    "    pad = np.concatenate(np.repeat([use_pad], PARAGRAPH_QTY-document.shape[0], axis=0), axis=0)\n",
    "    padded_documents.append(tf.concat([document, pad], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T19:12:42.389113400Z",
     "start_time": "2023-11-25T19:12:41.572340400Z"
    }
   },
   "outputs": [],
   "source": [
    "Xn_tensor = tf.convert_to_tensor(padded_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_Xn_use = tf.io.serialize_tensor(Xn_tensor)\n",
    "with open('use_padded.tensor', 'wb') as file:\n",
    "    file.write(serialized_Xn_use.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_use = Sequential()\n",
    "model_use.add(Bidirectional(LSTM(512, return_sequences=False, input_shape=(PARAGRAPH_QTY, 512))))\n",
    "model_use.add(Dense(256, activation='relu'))\n",
    "model_use.add(Dense(4, activation='softmax'))\n",
    "model_use.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 37s 37s/step - loss: 1.3989 - accuracy: 0.2645 - auc: 0.3664 - val_loss: 1.2143 - val_accuracy: 0.4906 - val_auc: 0.7458\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.2363 - accuracy: 0.4628 - auc: 0.7212 - val_loss: 1.1156 - val_accuracy: 0.4906 - val_auc: 0.7491\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 33s 33s/step - loss: 1.1970 - accuracy: 0.4628 - auc: 0.7285 - val_loss: 1.0880 - val_accuracy: 0.4906 - val_auc: 0.7542\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_6 (Bidirectio  (None, 1024)             4198400   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,461,828\n",
      "Trainable params: 4,461,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_use_total = clone_model(model_use)\n",
    "model_use_total.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"AUC\"])\n",
    "model_use_total.fit(Xn_tensor, labels_total, validation_split=0.3, epochs=3, batch_size=BATCH_SIZE)\n",
    "model_use_total.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 38s 38s/step - loss: 1.3866 - accuracy: 0.3140 - auc: 0.5046 - val_loss: 1.2434 - val_accuracy: 0.4906 - val_auc: 0.7203\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 34s 34s/step - loss: 1.2392 - accuracy: 0.4545 - auc: 0.7370 - val_loss: 1.1592 - val_accuracy: 0.4906 - val_auc: 0.7203\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 34s 34s/step - loss: 1.1449 - accuracy: 0.4545 - auc: 0.7444 - val_loss: 1.1549 - val_accuracy: 0.4906 - val_auc: 0.7315\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_6 (Bidirectio  (None, 1024)             4198400   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,461,828\n",
      "Trainable params: 4,461,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_use_contenido = clone_model(model_use)\n",
    "model_use_contenido.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"AUC\"])\n",
    "model_use_contenido.fit(Xn_tensor, labels_contenido, validation_split=0.3, epochs=3, batch_size=BATCH_SIZE)\n",
    "model_use_contenido.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación de documentos con Longformer - Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T02:21:56.474241800Z",
     "start_time": "2023-11-26T02:21:36.343354700Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/longformer-base-4096-spanish were not used when initializing RobertaModel: ['roberta.encoder.layer.11.attention.self.value_global.weight', 'roberta.encoder.layer.11.attention.self.query_global.weight', 'lm_head.bias', 'roberta.encoder.layer.11.attention.self.value_global.bias', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.2.attention.self.query_global.weight', 'roberta.encoder.layer.0.attention.self.query_global.bias', 'roberta.encoder.layer.8.attention.self.query_global.weight', 'roberta.encoder.layer.5.attention.self.value_global.bias', 'roberta.encoder.layer.2.attention.self.value_global.weight', 'roberta.encoder.layer.1.attention.self.query_global.bias', 'roberta.encoder.layer.1.attention.self.key_global.bias', 'roberta.encoder.layer.4.attention.self.query_global.bias', 'roberta.encoder.layer.5.attention.self.key_global.weight', 'roberta.encoder.layer.4.attention.self.key_global.weight', 'roberta.encoder.layer.3.attention.self.value_global.bias', 'roberta.encoder.layer.2.attention.self.query_global.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.1.attention.self.value_global.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.10.attention.self.key_global.bias', 'roberta.encoder.layer.3.attention.self.query_global.bias', 'roberta.encoder.layer.10.attention.self.value_global.bias', 'roberta.encoder.layer.7.attention.self.value_global.weight', 'roberta.encoder.layer.7.attention.self.value_global.bias', 'roberta.encoder.layer.9.attention.self.query_global.weight', 'roberta.encoder.layer.11.attention.self.query_global.bias', 'roberta.encoder.layer.7.attention.self.key_global.weight', 'roberta.encoder.layer.7.attention.self.key_global.bias', 'roberta.encoder.layer.6.attention.self.key_global.bias', 'roberta.encoder.layer.8.attention.self.query_global.bias', 'roberta.encoder.layer.2.attention.self.key_global.weight', 'roberta.encoder.layer.7.attention.self.query_global.weight', 'roberta.encoder.layer.5.attention.self.key_global.bias', 'roberta.encoder.layer.5.attention.self.query_global.bias', 'roberta.encoder.layer.10.attention.self.key_global.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.8.attention.self.key_global.weight', 'roberta.encoder.layer.3.attention.self.key_global.bias', 'roberta.encoder.layer.4.attention.self.value_global.bias', 'roberta.encoder.layer.0.attention.self.query_global.weight', 'roberta.encoder.layer.2.attention.self.key_global.bias', 'roberta.encoder.layer.11.attention.self.key_global.bias', 'roberta.encoder.layer.9.attention.self.query_global.bias', 'roberta.encoder.layer.9.attention.self.value_global.bias', 'roberta.encoder.layer.0.attention.self.value_global.weight', 'roberta.encoder.layer.8.attention.self.value_global.weight', 'roberta.encoder.layer.4.attention.self.query_global.weight', 'roberta.encoder.layer.0.attention.self.key_global.weight', 'roberta.encoder.layer.9.attention.self.value_global.weight', 'roberta.encoder.layer.7.attention.self.query_global.bias', 'roberta.encoder.layer.6.attention.self.query_global.bias', 'roberta.encoder.layer.3.attention.self.key_global.weight', 'roberta.encoder.layer.1.attention.self.key_global.weight', 'roberta.encoder.layer.6.attention.self.value_global.bias', 'roberta.encoder.layer.3.attention.self.query_global.weight', 'roberta.encoder.layer.0.attention.self.key_global.bias', 'roberta.encoder.layer.10.attention.self.query_global.weight', 'roberta.encoder.layer.3.attention.self.value_global.weight', 'roberta.encoder.layer.4.attention.self.key_global.bias', 'roberta.encoder.layer.10.attention.self.value_global.weight', 'roberta.encoder.layer.1.attention.self.query_global.weight', 'roberta.encoder.layer.5.attention.self.query_global.weight', 'roberta.encoder.layer.8.attention.self.value_global.bias', 'roberta.encoder.layer.4.attention.self.value_global.weight', 'roberta.encoder.layer.11.attention.self.key_global.weight', 'roberta.encoder.layer.10.attention.self.query_global.bias', 'roberta.encoder.layer.6.attention.self.key_global.weight', 'roberta.encoder.layer.2.attention.self.value_global.bias', 'roberta.encoder.layer.1.attention.self.value_global.bias', 'roberta.encoder.layer.8.attention.self.key_global.bias', 'roberta.encoder.layer.9.attention.self.key_global.bias', 'roberta.encoder.layer.0.attention.self.value_global.bias', 'roberta.encoder.layer.5.attention.self.value_global.weight', 'roberta.encoder.layer.6.attention.self.value_global.weight', 'roberta.encoder.layer.9.attention.self.key_global.weight', 'roberta.encoder.layer.6.attention.self.query_global.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at mrm8488/longformer-base-4096-spanish and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "long_tokenizer = RobertaTokenizer.from_pretrained(\"mrm8488/longformer-base-4096-spanish\",)\n",
    "long_model = RobertaModel.from_pretrained(\"mrm8488/longformer-base-4096-spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T02:21:56.574171Z",
     "start_time": "2023-11-26T02:21:56.436215900Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "2023-11-26 10:13:42.192975: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-11-26 10:13:42.193040: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-11-26 10:13:42.193071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cn04): /proc/driver/nvidia/version does not exist\n",
      "2023-11-26 10:13:42.193553: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inputs = long_tokenizer(\"\", return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = long_model(**inputs)\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "long_pad = tf.convert_to_tensor(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T04:11:54.541245Z",
     "start_time": "2023-11-26T04:11:54.474488300Z"
    }
   },
   "outputs": [],
   "source": [
    "def long_embedder(document):\n",
    "    embeddings = []\n",
    "    for page in document:\n",
    "        inputs = long_tokenizer(page, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = long_model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings.append(cls_embeddings)\n",
    "    print(\"done document\")\n",
    "    return tf.convert_to_tensor(torch.cat(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-26T04:11:59.260070400Z"
    },
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n"
     ]
    }
   ],
   "source": [
    "Xn_long_embed = [long_embedder(document) for document in dataset['block_documents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "padded_long_documents = []\n",
    "for document in Xn_long_embed:\n",
    "    pad = np.concatenate(np.repeat([long_pad], PARAGRAPH_QTY-document.shape[0], axis=0), axis=0)\n",
    "    padded_long_documents.append(tf.concat([document, pad], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "Xn_long_tensor = tf.convert_to_tensor(padded_long_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_Xn_long = tf.io.serialize_tensor(Xn_long_tensor)\n",
    "with open('long_padded.tensor', 'wb') as file:\n",
    "    file.write(serialized_Xn_long.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model_long = Sequential()\n",
    "model_long.add(Bidirectional(LSTM(512, return_sequences=False, input_shape=(PARAGRAPH_QTY, 768))))\n",
    "model_long.add(Dense(256, activation='relu'))\n",
    "model_long.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 48s 48s/step - loss: 1.3880 - accuracy: 0.2975 - auc: 0.5450 - val_loss: 2.8380 - val_accuracy: 0.4906 - val_auc: 0.7009\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 40s 40s/step - loss: 3.0274 - accuracy: 0.4628 - auc: 0.6995 - val_loss: 2.7869 - val_accuracy: 0.2453 - val_auc: 0.6428\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 40s 40s/step - loss: 2.8410 - accuracy: 0.2645 - auc: 0.6702 - val_loss: 1.3686 - val_accuracy: 0.2642 - val_auc: 0.5846\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 1024)             5246976   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,510,404\n",
      "Trainable params: 5,510,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_long_total = clone_model(model_long)\n",
    "model_long_total.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"AUC\"])\n",
    "model_long_total.fit(Xn_long_tensor, labels_total, validation_split=0.3, epochs=3, batch_size=BATCH_SIZE)\n",
    "model_long_total.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 44s 44s/step - loss: 1.2871 - accuracy: 0.4050 - auc: 0.6807 - val_loss: 2.3082 - val_accuracy: 0.4906 - val_auc: 0.7352\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 40s 40s/step - loss: 2.2955 - accuracy: 0.4545 - auc: 0.7476 - val_loss: 2.2444 - val_accuracy: 0.2642 - val_auc: 0.5790\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 40s 40s/step - loss: 2.0162 - accuracy: 0.2397 - auc: 0.6064 - val_loss: 2.2170 - val_accuracy: 0.2264 - val_auc: 0.5608\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 1024)             5246976   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,510,404\n",
      "Trainable params: 5,510,404\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_long_contenido = clone_model(model_long)\n",
    "model_long_contenido.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"AUC\"])\n",
    "model_long_contenido.fit(Xn_long_tensor, labels_contenido, validation_split=0.3, epochs=3, batch_size=BATCH_SIZE)\n",
    "model_long_contenido.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación de documentos con Tulio BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T12:36:48.846312700Z",
     "start_time": "2023-11-26T12:36:46.238052200Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/tulio-chilean-spanish-bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/tulio-chilean-spanish-bert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tulio_tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/tulio-chilean-spanish-bert\")\n",
    "tulio_model = AutoModel.from_pretrained(\"dccuchile/tulio-chilean-spanish-bert\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import pipeline\n",
    "tulio_pipe = pipeline(\"fill-mask\", model=\"dccuchile/tulio-chilean-spanish-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "inputs = tulio_tokenizer(\"\", return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = tulio_model(**inputs)\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "tulio_pad = tf.convert_to_tensor(cls_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def tulio_embedder(document):\n",
    "    embeddings = []\n",
    "    for page in document:\n",
    "        inputs = tulio_tokenizer(page, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = tulio_model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings.append(cls_embeddings)\n",
    "    print(\"done document\")\n",
    "    return tf.convert_to_tensor(torch.cat(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T12:36:58.773116300Z",
     "start_time": "2023-11-26T12:36:51.133611800Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n"
     ]
    }
   ],
   "source": [
    "Xn_tulio_embed = [tulio_embedder(document) for document in dataset['block_documents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "padded_tulio_documents = []\n",
    "for document in Xn_tulio_embed:\n",
    "    pad = np.concatenate(np.repeat([tulio_pad], PARAGRAPH_QTY-document.shape[0], axis=0), axis=0)\n",
    "    padded_tulio_documents.append(tf.concat([document, pad], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "Xn_tulio_tensor = tf.convert_to_tensor(padded_tulio_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_Xn_tulio = tf.io.serialize_tensor(Xn_tulio_tensor)\n",
    "with open('tulio_padded.tensor', 'wb') as file:\n",
    "    file.write(serialized_Xn_tulio.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model_tulio = Sequential()\n",
    "model_tulio.add(Bidirectional(LSTM(512, return_sequences=True, input_shape=(PARAGRAPH_QTY, 768))))\n",
    "model_tulio.add(Bidirectional(LSTM(units=512, return_sequences=False)))\n",
    "model_tulio.add(Dense(256, activation='relu'))\n",
    "model_tulio.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7fdccd5b5550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7fdccd5b5550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 1.4482 - accuracy: 0.2479 - auc: 0.4334WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fdccd5b5940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fdccd5b5940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 112s 112s/step - loss: 1.4482 - accuracy: 0.2479 - auc: 0.4334 - val_loss: 3.1288 - val_accuracy: 0.4906 - val_auc: 0.7070\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 118s 118s/step - loss: 3.2921 - accuracy: 0.4628 - auc: 0.6824 - val_loss: 2.0846 - val_accuracy: 0.2453 - val_auc: 0.5743\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 122s 122s/step - loss: 2.0777 - accuracy: 0.2645 - auc: 0.5907 - val_loss: 1.6179 - val_accuracy: 0.2642 - val_auc: 0.6811\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_7 (Bidirectio  (None, 2048, 1024)       5246976   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 1024)             6295552   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,805,956\n",
      "Trainable params: 11,805,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_tulio_total = clone_model(model_tulio)\n",
    "model_tulio_total.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"AUC\"])\n",
    "model_tulio_total.fit(Xn_tulio_tensor, labels_total, validation_split=0.3, epochs=3, batch_size=BATCH_SIZE)\n",
    "model_tulio_total.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "is_executing": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7fdcbc0e34c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x7fdcbc0e34c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 1.4137 - accuracy: 0.2397 - auc: 0.4393WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fdccd51fe50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fdccd51fe50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 118s 118s/step - loss: 1.4137 - accuracy: 0.2397 - auc: 0.4393 - val_loss: 2.2039 - val_accuracy: 0.4906 - val_auc: 0.7296\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 96s 96s/step - loss: 2.1685 - accuracy: 0.4545 - auc: 0.7410 - val_loss: 2.4357 - val_accuracy: 0.2264 - val_auc: 0.5625\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 106s 106s/step - loss: 2.1950 - accuracy: 0.2975 - auc: 0.6172 - val_loss: 1.5893 - val_accuracy: 0.2642 - val_auc: 0.5721\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_7 (Bidirectio  (None, 2048, 1024)       5246976   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 1024)             6295552   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,805,956\n",
      "Trainable params: 11,805,956\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_tulio_contenido = clone_model(model_tulio)\n",
    "model_tulio_contenido.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"AUC\"])\n",
    "model_tulio_contenido.fit(Xn_tulio_tensor, labels_contenido, validation_split=0.3, epochs=3, batch_size=BATCH_SIZE)\n",
    "model_tulio_contenido.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-26T12:36:37.140539300Z",
     "start_time": "2023-11-26T12:36:37.072806300Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
