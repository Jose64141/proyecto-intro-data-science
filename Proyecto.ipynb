{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "!python -m pip install pymupdf pandas openpyxl tensorflow tensorflow_hub tensorflow-text spacy classy-classification "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "!python -m spacy download es_dep_news_trf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Resumen Ejecutivo\n",
    "Durante el proceso de revisión de los informes de práctica del DISC (Departamento de Ingeniería de Sistemas y Computación), se requiere una inversión considerable de tiempo que, hasta la fecha, no ha sido automatizada. Esto conlleva largas jornadas de trabajo y carga adicional para los académicos, quienes podrían emplear ese tiempo en otras labores. Por lo tanto, como equipo de trabajo, hemos llegado a un consenso en la necesidad de llevar a cabo el análisis y desarrollo de un modelo que permita clasificar los informes en las categorías definidas en la rúbrica actual (insatisfactorio, regular, bueno y excelente).\n",
    "Es importante destacar que, con la llegada de la pandemia, la entrega de informes ha sido en formato digital, lo que ha generado un conjunto de aproximadamente 100 informes disponibles. Esta digitalización ofrece ventajas significativas para el entrenamiento del modelo, ya que se dispone de datos de entrada y resultados concretos (informe, rúbrica y nota).\n"
   ],
   "metadata": {
    "id": "wwxgY6M8rIDS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#pd.set_option(\"mode.copy_on_write\", True) #not on Python 3.9"
   ],
   "metadata": {
    "id": "50OQQBE7rlzE",
    "executionInfo": {
     "status": "error",
     "timestamp": 1698963780743,
     "user_tz": 180,
     "elapsed": 275,
     "user": {
      "displayName": "DAVID NAHUM ARAYA CADIZ",
      "userId": "08643668274056945811"
     }
    },
    "outputId": "a1ca34b1-97f5-4af3-affd-d781bb3f766b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "ExecuteTime": {
     "end_time": "2023-11-26T02:19:36.643268100Z",
     "start_time": "2023-11-26T02:19:35.080739100Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lectura de datos\n"
   ],
   "metadata": {
    "id": "GBIB6u4nrnD-",
    "ExecuteTime": {
     "end_time": "2023-10-10T23:40:00.959926900Z",
     "start_time": "2023-10-10T23:40:00.944208Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_classification(grade, number=False):\n",
    "  classification = [0,0,0] # Regular, Bueno, Excelente (Todo 0 = Insatisfactorio)\n",
    "  grade = round(grade, 1)\n",
    "  if(grade < 4):\n",
    "    return \"insatisfactorio\" if not number else 0\n",
    "  elif (4 <= grade < 5.5):\n",
    "    classification[0] = 1\n",
    "    return \"regular\" if not number else 1\n",
    "  elif (5.5 <= grade < 6.5):\n",
    "    classification[1] = 1\n",
    "    return \"bueno\" if not number else 2\n",
    "  elif (6.5 <= grade <= 7):\n",
    "    classification[2] = 1\n",
    "    return \"excelente\" if not number else 3"
   ],
   "metadata": {
    "id": "BKmoSmVHzB3W",
    "ExecuteTime": {
     "end_time": "2023-11-26T02:19:36.671363800Z",
     "start_time": "2023-11-26T02:19:36.589432300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(\"calificaciones.xlsx\", decimal=',')\n",
    "grades_columns = dataset.columns.difference([\"id\", \"periodo\", \"Unnamed: 9\"]) #[\"estructura\", \"escritura\", \"contenido\", \"conclusiones\", \"conocimiento\", \"relevancia\", \"total\"]\n",
    "rubric_columns = grades_columns.difference([\"total\"]) #, \"escritura\", \"estructura\"\n",
    "dataset = dataset.dropna(subset=grades_columns)"
   ],
   "metadata": {
    "id": "ALsO6MCgzB3W",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1698959147784,
     "user_tz": 180,
     "elapsed": 262,
     "user": {
      "displayName": "DAVID NAHUM ARAYA CADIZ",
      "userId": "08643668274056945811"
     }
    },
    "outputId": "93af515f-9b20-4e0f-c860-eeb9dae4dc71",
    "ExecuteTime": {
     "end_time": "2023-11-26T02:19:38.586138500Z",
     "start_time": "2023-11-26T02:19:36.589432300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extracción y limpieza de documentos\n",
    "En esta sección, se cargan los documentos en formato PDF, para la extracción y limpieza de estos, seguido de su integración al dataset."
   ],
   "metadata": {
    "collapsed": false,
    "id": "bHe-8AbczB3X"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "documents = []\n",
    "\n",
    "for id in dataset['id']:\n",
    "    pdf_file = fitz.open(f\"dataset/{id}.pdf\")\n",
    "    document_text = chr(12).join([page.get_text() for page in pdf_file])\n",
    "    documents.append(document_text)\n",
    "\n",
    "dataset.insert(loc=2, column=\"documents\", value=documents)\n",
    "dataset"
   ],
   "metadata": {
    "id": "5oOHwTYSry49",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1696970321635,
     "user_tz": 180,
     "elapsed": 9,
     "user": {
      "displayName": "JOSE MANUEL ALCAYAGA MARIN",
      "userId": "01460857505703722093"
     }
    },
    "outputId": "4a3d3351-aef6-452e-ddc5-034301bc4eea",
    "ExecuteTime": {
     "end_time": "2023-11-26T02:20:08.715233500Z",
     "start_time": "2023-11-26T02:19:38.649681500Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "             id periodo                                          documents  \\\n0    20908397-1  2023-1   \\n  \\nUNIVERSIDAD CATÓLICA DEL NORTE  \\nFACUL...   \n1    18971994-1  2023-1   \\nAntofagasta \\n \\n           Abril de 2023 \\...   \n2    19445943-1  2023-1   \\n1 \\n \\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\n...   \n5    19463712-1  2023-1   \\n \\n \\n \\n                                  ...   \n6    20218430-1  2023-1   \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...   \n..          ...     ...                                                ...   \n177  19928371-1  2021-1   \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...   \n178  19952605-1  2021-1   \\n \\n \\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nF...   \n179  19957163-1  2021-1   \\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTA...   \n180  20180533-1  2021-1   \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...   \n181  20408302-1  2021-1   \\ni \\n \\nx \\n \\nUNIVERSIDAD CATÓLICA DEL NORT...   \n\n     estructura  escritura  contenido  conclusiones  conocimiento  relevancia  \\\n0           6.2        5.1        6.0           5.5           4.4         6.0   \n1           6.9        6.8        6.8           6.8           7.0         6.9   \n2           6.7        6.9        6.5           6.4           6.8         7.0   \n5           4.4        4.9        4.9           5.8           4.0         6.0   \n6           6.1        5.8        5.5           5.0           4.5         5.8   \n..          ...        ...        ...           ...           ...         ...   \n177         6.8        6.2        6.0           5.7           6.3         6.0   \n178         7.0        6.8        6.8           7.0           7.0         7.0   \n179         6.5        4.5        5.8           5.5           6.4         6.3   \n180         7.0        6.0        6.7           6.8           6.8         7.0   \n181         6.5        5.0        6.0           6.5           6.5         6.5   \n\n     total Unnamed: 9  \n0      5.3        NaN  \n1      6.9        NaN  \n2      6.7        NaN  \n5      4.9        NaN  \n6      5.2        NaN  \n..     ...        ...  \n177    6.1        NaN  \n178    6.9        NaN  \n179    5.8        NaN  \n180    6.7        NaN  \n181    6.2        NaN  \n\n[174 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>periodo</th>\n      <th>documents</th>\n      <th>estructura</th>\n      <th>escritura</th>\n      <th>contenido</th>\n      <th>conclusiones</th>\n      <th>conocimiento</th>\n      <th>relevancia</th>\n      <th>total</th>\n      <th>Unnamed: 9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20908397-1</td>\n      <td>2023-1</td>\n      <td>\\n  \\nUNIVERSIDAD CATÓLICA DEL NORTE  \\nFACUL...</td>\n      <td>6.2</td>\n      <td>5.1</td>\n      <td>6.0</td>\n      <td>5.5</td>\n      <td>4.4</td>\n      <td>6.0</td>\n      <td>5.3</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18971994-1</td>\n      <td>2023-1</td>\n      <td>\\nAntofagasta \\n \\n           Abril de 2023 \\...</td>\n      <td>6.9</td>\n      <td>6.8</td>\n      <td>6.8</td>\n      <td>6.8</td>\n      <td>7.0</td>\n      <td>6.9</td>\n      <td>6.9</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19445943-1</td>\n      <td>2023-1</td>\n      <td>\\n1 \\n \\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\n...</td>\n      <td>6.7</td>\n      <td>6.9</td>\n      <td>6.5</td>\n      <td>6.4</td>\n      <td>6.8</td>\n      <td>7.0</td>\n      <td>6.7</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>19463712-1</td>\n      <td>2023-1</td>\n      <td>\\n \\n \\n \\n                                  ...</td>\n      <td>4.4</td>\n      <td>4.9</td>\n      <td>4.9</td>\n      <td>5.8</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>4.9</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>20218430-1</td>\n      <td>2023-1</td>\n      <td>\\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...</td>\n      <td>6.1</td>\n      <td>5.8</td>\n      <td>5.5</td>\n      <td>5.0</td>\n      <td>4.5</td>\n      <td>5.8</td>\n      <td>5.2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>19928371-1</td>\n      <td>2021-1</td>\n      <td>\\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...</td>\n      <td>6.8</td>\n      <td>6.2</td>\n      <td>6.0</td>\n      <td>5.7</td>\n      <td>6.3</td>\n      <td>6.0</td>\n      <td>6.1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>19952605-1</td>\n      <td>2021-1</td>\n      <td>\\n \\n \\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nF...</td>\n      <td>7.0</td>\n      <td>6.8</td>\n      <td>6.8</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>6.9</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>19957163-1</td>\n      <td>2021-1</td>\n      <td>\\n \\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTA...</td>\n      <td>6.5</td>\n      <td>4.5</td>\n      <td>5.8</td>\n      <td>5.5</td>\n      <td>6.4</td>\n      <td>6.3</td>\n      <td>5.8</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>20180533-1</td>\n      <td>2021-1</td>\n      <td>\\nUNIVERSIDAD CATÓLICA DEL NORTE \\nFACULTAD D...</td>\n      <td>7.0</td>\n      <td>6.0</td>\n      <td>6.7</td>\n      <td>6.8</td>\n      <td>6.8</td>\n      <td>7.0</td>\n      <td>6.7</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>20408302-1</td>\n      <td>2021-1</td>\n      <td>\\ni \\n \\nx \\n \\nUNIVERSIDAD CATÓLICA DEL NORT...</td>\n      <td>6.5</td>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>6.5</td>\n      <td>6.5</td>\n      <td>6.5</td>\n      <td>6.2</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>174 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocesamiento de etiquetas\n",
    "Con base en las notas obtenidas por cada entrada, se clasifican los documentos en las categorías definidas en la rúbrica.\n",
    "Se consideran dos tipos de etiquetas: texto y número. El primero se utiliza para el entrenamiento de modelos de Deep Learning, mientras que el segundo se utiliza para el entrenamiento de modelos tradicionales."
   ],
   "metadata": {
    "collapsed": false,
    "id": "4Q4tHjIzzB3Y"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "text_labeled_dataset = dataset.copy()\n",
    "text_labeled_dataset.loc[:, grades_columns] = text_labeled_dataset.loc[:, grades_columns].apply(lambda s: s.apply(get_classification))\n",
    "dataset.loc[:, grades_columns] = dataset.loc[:, grades_columns].apply(lambda s: s.apply(lambda x: get_classification(grade=x, number=True)))"
   ],
   "metadata": {
    "id": "mleH6J-3zB3Y",
    "outputId": "20495e2d-9827-4989-c04d-c8cb67b5429b",
    "ExecuteTime": {
     "end_time": "2023-11-26T02:20:08.716495200Z",
     "start_time": "2023-11-26T02:20:08.662301700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clasificación de documentos con métodos tradicionales\n",
    "Se usará SciKit-Learn para el análisis de los documentos mediante métodos tradicionales de NLP, como TF-IDF ~~y Word2Vec?~~. Se utilizará un modelo de clasificación de regresión logística y SVM para la clasificación de los documentos.\n",
    "En primera instancia, solo se probará la clasificiación final."
   ],
   "metadata": {
    "collapsed": false,
    "id": "MYMJnSAdzB3Z"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, CategoricalNB\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ],
   "metadata": {
    "id": "1lNHqqk-zB3Z",
    "ExecuteTime": {
     "end_time": "2023-11-26T02:20:22.513360500Z",
     "start_time": "2023-11-26T02:20:08.708074400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocesamiento TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "Xn = dataset['documents']\n",
    "yn = dataset[grades_columns]\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xn, yn, random_state=3, test_size=0.3)"
   ],
   "metadata": {
    "id": "BHDzcXQXzB3Z",
    "ExecuteTime": {
     "end_time": "2023-11-26T02:20:22.574858300Z",
     "start_time": "2023-11-26T02:20:22.574858300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.9, min_df=0.2)\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)"
   ],
   "metadata": {
    "id": "z0HQfhUBzB3a",
    "ExecuteTime": {
     "end_time": "2023-11-13T13:31:56.686125200Z",
     "start_time": "2023-11-13T13:31:53.586042500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regresores de nota final"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def get_final_grade(rubric_grades):\n",
    "    result = rubric_grades['estructura']*0.5\n",
    "    result += rubric_grades['escritura']*0.15\n",
    "    result += rubric_grades['contenido']*0.25\n",
    "    result += rubric_grades['conclusiones']*0.15\n",
    "    result += rubric_grades['conocimiento']*0.30\n",
    "    result += rubric_grades['relevancia']*0.10\n",
    "    return round(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T14:08:35.651595500Z",
     "start_time": "2023-11-13T14:08:35.634519600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10344827586206896\n"
     ]
    },
    {
     "data": {
      "text/plain": "0      2.0\n1      4.0\n2      4.0\n5      2.0\n6      2.0\n      ... \n177    3.0\n178    4.0\n179    3.0\n180    4.0\n181    4.0\nName: estructura, Length: 174, dtype: float64"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_calc = get_final_grade(yn[rubric_columns])\n",
    "print(accuracy_score(y_calc, yn['total']))\n",
    "y_calc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T14:11:21.995464600Z",
     "start_time": "2023-11-13T14:11:21.919605700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8113207547169812\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'Característica': Index(['conclusiones', 'conocimiento', 'contenido', 'escritura', 'estructura',\n        'relevancia'],\n       dtype='object'),\n 'Importancia': array([0.16857424, 0.2603969 , 0.26508912, 0.19325956, 0.05499339,\n        0.05768678])}"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_grade_rf = RandomForestClassifier()\n",
    "final_grade_rf.fit(y_train[rubric_columns], y_train['total'])\n",
    "grade_pred_rf = final_grade_rf.predict(y_test[rubric_columns])\n",
    "print(accuracy_score(y_test['total'], grade_pred_rf))\n",
    "{'Característica': rubric_columns, 'Importancia': final_grade_rf.feature_importances_}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T14:01:40.439103800Z",
     "start_time": "2023-11-13T14:01:40.314202500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8679245283018868\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'Característica': Index(['conclusiones', 'conocimiento', 'contenido', 'escritura', 'estructura',\n        'relevancia'],\n       dtype='object'),\n 'Importancia': array([[-1.34695751, -1.35070132, -0.92570698, -1.02865765, -0.25962928,\n         -1.29684428],\n        [-0.59156168, -1.05596707, -0.89826997, -1.19481917, -0.23909476,\n         -0.17719915],\n        [ 0.46610424,  0.36809972,  0.03083842,  0.40622851, -0.19707045,\n          0.37408921],\n        [ 1.47241495,  2.03856867,  1.79313853,  1.8172483 ,  0.69579448,\n          1.09995422]])}"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_grade_log = LogisticRegression(max_iter=1000)\n",
    "final_grade_log.fit(y_train[rubric_columns], y_train['total'])\n",
    "grade_pred_log = final_grade_log.predict(y_test[rubric_columns])\n",
    "print(accuracy_score(y_test['total'], grade_pred_log))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T14:01:41.649059400Z",
     "start_time": "2023-11-13T14:01:41.602514400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CategoricalNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m final_grade_nb \u001B[38;5;241m=\u001B[39m \u001B[43mCategoricalNB\u001B[49m()\n\u001B[0;32m      2\u001B[0m final_grade_nb\u001B[38;5;241m.\u001B[39mfit(y_train[rubric_columns], y_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtotal\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      3\u001B[0m grade_pred_nb \u001B[38;5;241m=\u001B[39m final_grade_log\u001B[38;5;241m.\u001B[39mpredict(y_test[rubric_columns])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'CategoricalNB' is not defined"
     ]
    }
   ],
   "source": [
    "final_grade_nb = CategoricalNB()\n",
    "final_grade_nb.fit(y_train[rubric_columns], y_train['total'])\n",
    "grade_pred_nb = final_grade_log.predict(y_test[rubric_columns])\n",
    "print(accuracy_score(y_test['total'], grade_pred_nb))\n",
    "print(f1_score(y_test['total'], grade_pred_nb, average='weighted'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T15:18:54.597476100Z",
     "start_time": "2023-11-13T15:18:54.546424800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clasificación de documentos con SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5849056603773585\n",
      "0.5516265369466873\n"
     ]
    }
   ],
   "source": [
    "svc_n = SVC(C=10)\n",
    "svc_n.fit(X_train_bow, y_train['total'])\n",
    "y_pred = svc_n.predict(X_test_bow)\n",
    "print(accuracy_score(y_test['total'], y_pred))\n",
    "print(f1_score(y_test['total'], y_pred, average='weighted'))"
   ],
   "metadata": {
    "id": "rs1HVwG_zB3a",
    "outputId": "38650c0d-f462-4168-966f-5f0b0dca9819",
    "ExecuteTime": {
     "end_time": "2023-11-13T14:39:07.734194600Z",
     "start_time": "2023-11-13T14:39:07.421751300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07547169811320754\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass-multioutput is not supported",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m y_pred_cont \u001B[38;5;241m=\u001B[39m svc_n_mo\u001B[38;5;241m.\u001B[39mpredict(X_test_bow)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(svc_n_mo\u001B[38;5;241m.\u001B[39mscore(X_test_bow, y_test[rubric_columns]))\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mf1_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_test\u001B[49m\u001B[43m[\u001B[49m\u001B[43mrubric_columns\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred_cont\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmacro\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      6\u001B[0m y_pred_cont\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    210\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    211\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    212\u001B[0m         )\n\u001B[0;32m    213\u001B[0m     ):\n\u001B[1;32m--> 214\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    220\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    224\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1239\u001B[0m, in \u001B[0;36mf1_score\u001B[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001B[0m\n\u001B[0;32m   1070\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m   1071\u001B[0m     {\n\u001B[0;32m   1072\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1097\u001B[0m     zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1098\u001B[0m ):\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \n\u001B[0;32m   1101\u001B[0m \u001B[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1237\u001B[0m \u001B[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001B[39;00m\n\u001B[0;32m   1238\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfbeta_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1240\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1241\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1244\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpos_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1245\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1246\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mzero_division\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mzero_division\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1248\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:187\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    185\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 187\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    189\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1413\u001B[0m, in \u001B[0;36mfbeta_score\u001B[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001B[0m\n\u001B[0;32m   1251\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m   1252\u001B[0m     {\n\u001B[0;32m   1253\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1280\u001B[0m     zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1281\u001B[0m ):\n\u001B[0;32m   1282\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute the F-beta score.\u001B[39;00m\n\u001B[0;32m   1283\u001B[0m \n\u001B[0;32m   1284\u001B[0m \u001B[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1410\u001B[0m \u001B[38;5;124;03m    0.38...\u001B[39;00m\n\u001B[0;32m   1411\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1413\u001B[0m     _, _, f, _ \u001B[38;5;241m=\u001B[39m \u001B[43mprecision_recall_fscore_support\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1414\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1415\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1416\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1417\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1418\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpos_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1419\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1420\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwarn_for\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mf-score\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1421\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1422\u001B[0m \u001B[43m        \u001B[49m\u001B[43mzero_division\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mzero_division\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1423\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:187\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    185\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 187\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    189\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1724\u001B[0m, in \u001B[0;36mprecision_recall_fscore_support\u001B[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001B[0m\n\u001B[0;32m   1566\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001B[39;00m\n\u001B[0;32m   1567\u001B[0m \n\u001B[0;32m   1568\u001B[0m \u001B[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1721\u001B[0m \u001B[38;5;124;03m array([2, 2, 2]))\u001B[39;00m\n\u001B[0;32m   1722\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1723\u001B[0m zero_division_value \u001B[38;5;241m=\u001B[39m _check_zero_division(zero_division)\n\u001B[1;32m-> 1724\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43m_check_set_wise_labels\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1726\u001B[0m \u001B[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001B[39;00m\n\u001B[0;32m   1727\u001B[0m samplewise \u001B[38;5;241m=\u001B[39m average \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msamples\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1501\u001B[0m, in \u001B[0;36m_check_set_wise_labels\u001B[1;34m(y_true, y_pred, average, labels, pos_label)\u001B[0m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m average \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m average_options \u001B[38;5;129;01mand\u001B[39;00m average \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1499\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maverage has to be one of \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(average_options))\n\u001B[1;32m-> 1501\u001B[0m y_type, y_true, y_pred \u001B[38;5;241m=\u001B[39m \u001B[43m_check_targets\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \u001B[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001B[39;00m\n\u001B[0;32m   1504\u001B[0m present_labels \u001B[38;5;241m=\u001B[39m unique_labels(y_true, y_pred)\u001B[38;5;241m.\u001B[39mtolist()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:104\u001B[0m, in \u001B[0;36m_check_targets\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001B[39;00m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_type \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultilabel-indicator\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m--> 104\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m is not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(y_type))\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_type \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    107\u001B[0m     y_true \u001B[38;5;241m=\u001B[39m column_or_1d(y_true)\n",
      "\u001B[1;31mValueError\u001B[0m: multiclass-multioutput is not supported"
     ]
    }
   ],
   "source": [
    "svc_n_mo = MultiOutputClassifier(svc_n)\n",
    "svc_n_mo.fit(X_train_bow, y_train[rubric_columns])\n",
    "y_pred_cont = svc_n_mo.predict(X_test_bow)\n",
    "print(svc_n_mo.score(X_test_bow, y_test[rubric_columns]))\n",
    "print(f1_score(y_test[rubric_columns], y_pred_cont, average='weighted'))\n",
    "y_pred_cont"
   ],
   "metadata": {
    "id": "yqKqRUmNzB3a",
    "outputId": "fbae29c6-e7fd-4556-9f6c-37ed6270250a",
    "ExecuteTime": {
     "end_time": "2023-11-13T14:37:59.441941100Z",
     "start_time": "2023-11-13T14:37:56.923915Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clasificación de documentos con Regresión Ridge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5660377358490566\n",
      "0.5275254892769249\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier()\n",
    "ridge.fit(X_train_bow, y_train['total'])\n",
    "y_pred_r = ridge.predict(X_test_bow)\n",
    "print(accuracy_score(y_test['total'], y_pred_r))\n",
    "print(f1_score(y_test['total'], y_pred_r, average='weighted'))"
   ],
   "metadata": {
    "id": "WTDlUHWwzB3a",
    "outputId": "fa85e054-e909-4275-c010-e2f1b5b3e067",
    "ExecuteTime": {
     "end_time": "2023-11-13T14:39:01.699203700Z",
     "start_time": "2023-11-13T14:39:01.628719100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1320754716981132\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[2, 1, 3, 2],\n       [3, 2, 2, 3],\n       [2, 2, 2, 2],\n       [3, 1, 2, 3],\n       [3, 3, 2, 3],\n       [3, 2, 2, 2],\n       [3, 3, 2, 3],\n       [2, 2, 2, 3],\n       [3, 3, 3, 3],\n       [3, 2, 2, 3],\n       [2, 1, 1, 3],\n       [2, 1, 2, 2],\n       [2, 3, 2, 3],\n       [2, 2, 2, 3],\n       [2, 2, 2, 3],\n       [3, 3, 1, 3],\n       [3, 1, 2, 2],\n       [2, 3, 2, 3],\n       [3, 2, 2, 3],\n       [3, 1, 1, 3],\n       [1, 2, 2, 2],\n       [1, 1, 1, 3],\n       [3, 2, 2, 3],\n       [3, 2, 2, 3],\n       [2, 1, 1, 2],\n       [3, 3, 3, 3],\n       [3, 3, 2, 3],\n       [3, 2, 2, 3],\n       [3, 3, 3, 3],\n       [2, 3, 2, 3],\n       [1, 2, 2, 2],\n       [3, 3, 2, 3],\n       [3, 3, 2, 2],\n       [3, 2, 2, 3],\n       [2, 2, 2, 2],\n       [3, 3, 2, 3],\n       [2, 3, 2, 3],\n       [3, 1, 2, 2],\n       [2, 2, 2, 3],\n       [3, 1, 2, 2],\n       [3, 3, 2, 3],\n       [3, 2, 2, 3],\n       [1, 2, 2, 2],\n       [3, 3, 2, 3],\n       [2, 3, 2, 3],\n       [2, 2, 2, 2],\n       [3, 2, 2, 3],\n       [2, 3, 2, 3],\n       [3, 3, 3, 3],\n       [2, 3, 2, 3],\n       [3, 1, 3, 2],\n       [2, 2, 2, 2],\n       [2, 2, 2, 2]], dtype=int64)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_mo = MultiOutputClassifier(ridge)\n",
    "ridge_mo.fit(X_train_bow, y_train[rubric_columns])\n",
    "y_pred_cont = ridge_mo.predict(X_test_bow)\n",
    "print(ridge_mo.score(X_test_bow, y_test[rubric_columns]))\n",
    "y_pred_cont"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T13:46:56.828983100Z",
     "start_time": "2023-11-13T13:46:56.698586700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clasificación de documentos con Regresión Logística"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6037735849056604\n",
      "0.5322023148882195\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(X_train_bow, y_train['total'])\n",
    "y_pred_l = log.predict(X_test_bow)\n",
    "print(accuracy_score(y_test['total'], y_pred_l))\n",
    "print(f1_score(y_test['total'], y_pred_l, average='weighted'))"
   ],
   "metadata": {
    "id": "5hAxzAUJzB3a",
    "outputId": "1871a340-3e03-4033-95d2-2259223f59a8",
    "ExecuteTime": {
     "end_time": "2023-11-13T14:38:53.331610400Z",
     "start_time": "2023-11-13T14:38:53.215161700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clasificación de documentos con Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6037735849056604\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_bow, y_train['total'])\n",
    "y_pred_rf = log.predict(X_test_bow)\n",
    "print(accuracy_score(y_test['total'], y_pred_l))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T13:31:58.787004700Z",
     "start_time": "2023-11-13T13:31:58.111578400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018867924528301886\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[2, 1, 2, 2, 2, 3],\n       [3, 1, 2, 1, 3, 3],\n       [3, 2, 2, 2, 3, 3],\n       [3, 1, 2, 2, 2, 3],\n       [3, 3, 2, 1, 3, 3],\n       [3, 3, 3, 2, 3, 3],\n       [3, 3, 2, 2, 3, 3],\n       [2, 2, 2, 2, 2, 3],\n       [2, 3, 2, 1, 3, 3],\n       [3, 1, 2, 1, 3, 3],\n       [2, 1, 2, 2, 3, 3],\n       [2, 3, 2, 2, 3, 3],\n       [3, 3, 2, 1, 3, 3],\n       [3, 2, 2, 2, 3, 3],\n       [2, 2, 2, 2, 3, 3],\n       [2, 1, 2, 1, 3, 3],\n       [3, 3, 2, 2, 3, 3],\n       [3, 3, 2, 1, 3, 3],\n       [3, 2, 2, 1, 3, 3],\n       [2, 1, 2, 1, 3, 3],\n       [1, 1, 2, 1, 2, 2],\n       [3, 3, 2, 2, 3, 3],\n       [3, 3, 2, 2, 3, 3],\n       [2, 3, 2, 2, 3, 2],\n       [2, 1, 2, 2, 3, 2],\n       [2, 2, 2, 1, 3, 3],\n       [3, 1, 2, 1, 3, 3],\n       [2, 3, 2, 2, 3, 3],\n       [2, 2, 3, 1, 3, 3],\n       [3, 3, 2, 2, 3, 3],\n       [3, 2, 2, 1, 3, 2],\n       [3, 2, 2, 1, 3, 3],\n       [3, 3, 2, 2, 3, 2],\n       [3, 2, 2, 1, 3, 3],\n       [2, 1, 1, 1, 3, 2],\n       [3, 3, 2, 2, 3, 3],\n       [3, 3, 2, 1, 3, 3],\n       [2, 1, 2, 2, 3, 3],\n       [2, 3, 2, 1, 3, 3],\n       [3, 1, 2, 1, 3, 3],\n       [3, 3, 2, 2, 3, 3],\n       [3, 1, 2, 2, 3, 3],\n       [2, 2, 2, 1, 3, 2],\n       [3, 3, 2, 2, 3, 3],\n       [2, 2, 2, 1, 3, 3],\n       [3, 1, 2, 1, 2, 2],\n       [3, 2, 2, 1, 2, 2],\n       [2, 1, 2, 1, 3, 3],\n       [3, 1, 2, 1, 3, 2],\n       [3, 3, 2, 1, 3, 3],\n       [3, 1, 2, 1, 3, 3],\n       [3, 3, 2, 2, 3, 3],\n       [2, 2, 2, 1, 2, 2]], dtype=int64)"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mo = MultiOutputClassifier(rf)\n",
    "rf_mo.fit(X_train_bow, y_train[rubric_columns])\n",
    "y_pred_cont = rf_mo.predict(X_test_bow)\n",
    "print(rf_mo.score(X_test_bow, y_test[rubric_columns]))\n",
    "y_pred_cont"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T14:25:11.122720300Z",
     "start_time": "2023-11-13T14:25:07.570772600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clasificación de documentos con Naïve Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5660377358490566\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_bow, y_train['total'])\n",
    "y_pred_nb = nb.predict(X_test_bow)\n",
    "print(accuracy_score(y_test['total'], y_pred_nb))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T13:33:42.643289600Z",
     "start_time": "2023-11-13T13:33:42.571578500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clasificación de documentos con XGBoost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clasificación de documentos con Deep Learning"
   ],
   "metadata": {
    "collapsed": false,
    "id": "HoRgmlrbzB3b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clasificación de documentos con Spacy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Xn_text_labeled = text_labeled_dataset['documents']\n",
    "yn_text_labeled = text_labeled_dataset['total']\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(Xn_text_labeled, yn_text_labeled, random_state=3, test_size=0.3)"
   ],
   "metadata": {
    "id": "HMvJNW6rzB3b",
    "ExecuteTime": {
     "start_time": "2023-11-13T13:29:54.204446200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_data_total = {\n",
    "  \"insatisfactorio\": [],\n",
    "  \"regular\": [],\n",
    "  \"bueno\": [],\n",
    "  \"excelente\": []\n",
    "}\n",
    "for index, document in X_text_train.items():\n",
    "  training_data_total[y_text_train[index]].append(document)\n",
    "training_data_total"
   ],
   "metadata": {
    "id": "22OHNVvlzB3b",
    "outputId": "d7606e11-05f2-4b6f-a4ff-c6c3a258c23a",
    "ExecuteTime": {
     "start_time": "2023-11-13T13:29:54.206970Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://spacy.io/universe/project/classyclassification"
   ],
   "metadata": {
    "collapsed": false,
    "id": "QOG-iZUXzB3b"
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "import spacy\n",
    "from spacy_transformers import Transformer\n",
    "from spacy_transformers.pipeline_component import DEFAULT_CONFIG\n",
    "nlp = spacy.load(\"es_dep_news_trf\")\n",
    "nlp.add_pipe(\"classy_classification\",\n",
    "    config={\n",
    "        \"data\": training_data_total,\n",
    "        \"model\": \"spacy\"\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "id": "mlVcxih6zB3b"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from classy_classification import ClassyClassifier\n",
    "classifier = ClassyClassifier(data=training_data_total)\n",
    "classifier.set_embedding_model(model=\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "y_text_pred = classifier.pipe(X_text_test.tolist())"
   ],
   "metadata": {
    "id": "26ki2BbFzB3e",
    "ExecuteTime": {
     "start_time": "2023-11-13T13:29:54.208980500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_text_pred"
   ],
   "metadata": {
    "id": "aGs2kN8XzB3f",
    "outputId": "764f471b-2fa4-473a-9019-7aa1aecc2cc7",
    "ExecuteTime": {
     "start_time": "2023-11-13T13:29:54.213068700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_text_test"
   ],
   "metadata": {
    "id": "Lr7IDg0SzB3f",
    "outputId": "621c6e2a-ccd6-49d9-fadc-6c175e9eedd4",
    "ExecuteTime": {
     "start_time": "2023-11-13T13:29:54.216093800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clasificación de documentos con TensorFlow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4438\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "max_length = -1\n",
    "for id in dataset['id']:\n",
    "    pdf_file = fitz.open(f\"dataset/{id}.pdf\")\n",
    "    document_text = [] \n",
    "    for page in pdf_file:\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        document_text += [block[4] for block in blocks]\n",
    "    documents.append(document_text)\n",
    "    for block in document_text:\n",
    "        if len(block) > max_length:\n",
    "            max_length = len(block)\n",
    "print(max_length)            \n",
    "dataset.insert(loc=2, column=\"block_documents\", value=documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T02:20:36.230073600Z",
     "start_time": "2023-11-26T02:20:22.574858300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 23:18:22.976503: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-25 23:18:24.900399: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-11-25 23:18:24.900512: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-11-25 23:18:41.298247: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-25 23:18:41.298810: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-11-25 23:18:41.298829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "from keras import utils\n",
    "import torch\n",
    "PARAGRAPH_QTY = 2048\n",
    "BATCH_SIZE= 171\n",
    "labels = utils.to_categorical(dataset['total'], num_classes=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T02:21:36.290176800Z",
     "start_time": "2023-11-26T02:20:36.170241800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clasificación de documentos con Universal Sentence Encoder - Multilingual Large\n",
    "La estructura de entrada corresponde a la siguiente:\n",
    "Cada documento se encuentra almacenado como una lista de párrafos.\n",
    "Así, la entrada del modelo LSTM sería un conjunto de lotes, correspondiendo a lo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 14:54:24.417375: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-11-25 14:54:24.417467: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-11-25 14:54:24.417524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cn05): /proc/driver/nvidia/version does not exist\n",
      "2023-11-25 14:54:24.418268: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "use_embedder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"
   ],
   "metadata": {
    "id": "e51hCbTczB3f",
    "ExecuteTime": {
     "end_time": "2023-11-25T17:57:43.889612700Z",
     "start_time": "2023-11-25T17:55:31.047558Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 512), dtype=float32, numpy=\narray([[ 9.67169367e-03, -2.54486930e-02, -3.17078345e-02,\n        -2.97019519e-02, -1.05562985e-01,  1.94779057e-02,\n         5.57217561e-03, -6.65287301e-02, -9.96011775e-03,\n        -2.87357699e-02, -4.46397811e-02, -3.81327048e-03,\n        -2.46620625e-02,  1.41643267e-02,  1.62112806e-02,\n         3.12007181e-02,  2.16443427e-02, -1.47441644e-02,\n        -3.72272567e-03, -1.33331269e-02,  9.22790088e-04,\n         6.21030889e-02,  4.95002009e-02, -7.31829880e-03,\n         9.38552152e-03, -6.95153605e-03,  1.25779901e-02,\n         1.24983238e-02, -7.82670360e-03, -4.84213866e-02,\n        -1.85677093e-02, -1.64676979e-02, -7.40418630e-03,\n        -5.79094924e-02, -1.73449591e-02, -6.26746789e-02,\n        -4.70428318e-02, -2.61945203e-02, -9.74262424e-04,\n        -8.06684420e-02,  3.92258726e-02, -1.62350815e-02,\n        -2.77670939e-02, -6.16709851e-02,  1.24697713e-02,\n        -3.87554914e-02, -1.99186374e-02,  5.62777333e-02,\n         6.54689595e-02, -3.16797756e-03, -2.01689471e-02,\n         8.51311255e-03, -5.01891933e-02, -3.13714817e-02,\n        -4.90770377e-02,  2.19256897e-03, -2.05077380e-02,\n         9.55089927e-03, -1.42786987e-02,  7.00806156e-02,\n         5.12170456e-02,  1.25995802e-03, -3.04883067e-02,\n         2.17546560e-02, -1.74804144e-02,  5.53701743e-02,\n        -5.23474589e-02, -8.56720563e-03,  6.95965579e-03,\n        -1.09773520e-02, -4.04201495e-03,  1.70518328e-02,\n        -3.01196873e-02,  8.13683793e-02, -9.31979809e-03,\n         3.10247429e-02,  4.81703356e-02,  1.15644597e-02,\n        -1.46793593e-02,  1.70515070e-03,  5.59435524e-02,\n        -5.49252555e-02, -2.61449963e-01,  3.37384827e-02,\n        -1.48430998e-02, -2.96787731e-02, -8.50612484e-03,\n         6.10537082e-02, -5.26770055e-02, -1.86347850e-02,\n        -1.01458402e-02,  1.76593140e-02, -1.32827044e-01,\n         3.67087536e-02, -1.41586214e-02,  2.69173861e-01,\n        -8.48926455e-02,  4.21778746e-02, -2.61233542e-02,\n        -3.93837392e-02,  1.20488172e-02,  8.28017667e-03,\n         2.92056687e-02,  5.02164587e-02,  9.88269784e-03,\n        -2.91151442e-02,  1.13544129e-02,  1.82307474e-02,\n        -7.34521076e-02, -6.76134368e-03, -6.99600950e-02,\n         1.03727445e-01, -3.53323221e-02,  4.90198843e-02,\n         9.69164446e-02, -3.72555628e-02,  4.91524413e-02,\n         9.74573340e-05,  1.15129789e-02,  1.11527424e-02,\n        -3.88014540e-02,  5.67408919e-04,  2.50749383e-02,\n        -2.48571746e-02,  2.67356485e-02, -3.72562394e-03,\n        -1.43551854e-02,  7.76987616e-03, -3.12412344e-02,\n         7.55694602e-03,  4.64329645e-02,  7.58228600e-02,\n         1.21107558e-02,  3.73519696e-02,  7.02384561e-02,\n         2.08270196e-02,  5.34108803e-02,  3.46711674e-03,\n        -6.96374401e-02, -6.43638223e-02,  3.76269296e-02,\n         2.78812386e-02,  1.10432561e-02,  4.33101086e-03,\n        -7.28747714e-03,  1.48335863e-02, -1.57567114e-03,\n        -7.86311517e-04,  2.68525500e-02,  1.31431166e-02,\n        -3.34219560e-02,  3.28002498e-02,  1.37576684e-02,\n         1.50606493e-02, -3.50905769e-02,  5.73289394e-03,\n        -1.21911410e-02,  1.17121544e-02,  4.21328619e-02,\n         2.80791223e-02,  2.18843017e-02, -3.80964838e-02,\n        -6.84505254e-02, -2.01830901e-02, -8.23430624e-03,\n         8.17687949e-04,  2.39122398e-02, -6.58514202e-02,\n        -3.40604130e-03,  2.13826206e-02, -3.62342000e-02,\n         2.16407791e-01, -2.33819466e-02, -2.70619281e-02,\n         2.51676911e-03, -2.18922272e-03,  2.64952369e-02,\n        -3.61987166e-02, -4.46625315e-02, -2.45975126e-02,\n         1.58178769e-02,  1.71396637e-03,  8.32801089e-02,\n        -1.77662093e-02, -3.64475176e-02,  3.04290876e-02,\n        -4.27458212e-02, -8.46528336e-02,  5.92607036e-02,\n        -1.07963286e-01,  2.09131856e-02, -4.54855859e-02,\n         9.19091478e-02, -7.55583635e-03, -2.75996141e-02,\n        -2.39302386e-02,  4.15010564e-03,  9.97123402e-03,\n        -3.84450331e-02,  4.36878651e-02, -5.23824943e-03,\n        -1.41904682e-01, -4.53576595e-02, -3.63857932e-02,\n         5.69371767e-02, -1.24294814e-02, -3.82532142e-02,\n        -7.01812506e-02, -9.91159771e-03,  5.30326506e-03,\n        -2.89066527e-02, -4.32573669e-02,  7.65134301e-03,\n         1.65611424e-03, -5.74219506e-03, -7.37054506e-03,\n         4.01764587e-02,  2.93930452e-02,  3.04219276e-02,\n         3.34856920e-02, -1.48805026e-02,  7.91213533e-04,\n        -2.39351578e-03,  3.20488550e-02,  4.60976409e-03,\n         2.17720922e-02,  3.33155319e-03, -2.03291848e-02,\n         5.32270670e-02,  1.08883521e-02,  3.64181325e-02,\n         8.41824710e-02, -4.75133136e-02,  2.14171596e-02,\n         1.50475726e-02, -2.21619662e-02,  1.48701090e-02,\n        -3.69255454e-03, -1.63131692e-02,  1.14656597e-01,\n        -4.11086017e-03, -5.28560467e-02,  1.30482744e-02,\n         7.80061353e-03, -1.56279989e-02, -1.69295527e-03,\n         4.49456386e-02, -1.69574451e-02,  2.86382791e-02,\n         8.81355330e-02,  5.17744422e-02,  4.89752628e-02,\n         8.10799152e-02, -2.09067632e-02,  1.81983560e-02,\n         4.42520455e-02, -5.37838712e-02, -6.39369637e-02,\n         6.72244933e-03,  1.86588448e-02,  6.22742670e-03,\n         2.66819503e-02,  3.20464969e-02,  1.82631090e-02,\n         2.75369268e-02,  4.24247347e-02,  1.72804780e-02,\n        -1.79740544e-02,  5.69692589e-02, -3.28610577e-02,\n        -2.73194239e-02,  5.28178699e-02, -1.39837069e-02,\n         2.43507586e-02,  5.18359318e-02, -1.19452151e-02,\n         1.38965948e-02,  4.52476293e-02, -1.23296247e-03,\n         6.18805476e-02, -4.04643901e-02,  5.67348190e-02,\n         1.97902392e-03, -1.48787387e-02,  2.63360096e-03,\n        -5.69725484e-02, -3.27048041e-02,  1.40043134e-02,\n         2.73061618e-02, -9.56247840e-03, -4.86216359e-02,\n        -5.48006371e-02,  2.21642070e-02, -4.58140485e-02,\n         2.09591407e-02, -1.74944010e-02,  1.46962255e-02,\n         2.75710262e-02, -6.18674047e-02,  8.54445528e-03,\n         5.67164905e-02,  1.27747990e-02, -3.35485116e-02,\n        -2.13837046e-02,  1.97878629e-02, -1.20869419e-02,\n         5.24243973e-02,  3.66286114e-02, -5.33835255e-02,\n         6.09421870e-03, -9.67514515e-02,  3.13836671e-02,\n        -1.55470548e-02, -8.17506239e-02,  6.99329935e-03,\n        -2.32852008e-02,  2.43563112e-02,  1.06839845e-02,\n        -4.86892313e-02,  1.12295905e-02,  4.58197929e-02,\n        -8.76399875e-02,  5.96032925e-02,  4.88436490e-04,\n         1.43603291e-02,  2.00533923e-02, -8.46898332e-02,\n         2.35904064e-02,  5.04308753e-02,  7.84202851e-03,\n        -7.44977454e-03,  1.27283886e-01, -1.35359392e-02,\n         1.89147349e-02, -4.03218903e-02, -1.59497242e-02,\n         1.74361523e-02, -1.21961366e-02,  5.54714538e-02,\n         3.68526811e-03, -4.53717709e-02, -1.28521482e-02,\n         1.08115245e-02, -2.05686875e-03, -2.00341679e-02,\n         2.43068542e-02, -1.40691837e-02,  1.94273144e-02,\n        -3.65311615e-02,  4.67473961e-04, -2.58706268e-02,\n        -1.98534504e-02, -9.98952016e-02,  1.54054537e-03,\n        -5.45017049e-02, -2.28644684e-02, -7.63047487e-02,\n         5.64195924e-02, -3.23107913e-02, -8.58988017e-02,\n        -8.98431335e-03, -3.14508844e-03, -5.23723438e-02,\n         3.86926867e-02,  2.52573229e-02,  9.63359978e-03,\n         1.23464586e-02,  2.76469458e-02,  1.35041149e-02,\n         4.64201495e-02,  3.22657675e-02, -1.87020805e-02,\n         1.21963788e-02, -2.01634951e-02, -3.48284170e-02,\n        -4.26750118e-03, -1.47185372e-02,  7.23658428e-02,\n        -3.93106304e-02,  7.42362533e-03,  1.27363969e-02,\n        -3.27389017e-02,  1.15371717e-03,  2.12804880e-02,\n         2.27804799e-02,  2.17587296e-02,  5.10678031e-02,\n        -4.56998087e-02, -2.12043803e-03, -2.49358658e-02,\n        -5.45168435e-03, -6.39962032e-03, -4.35556881e-02,\n         3.34644690e-02, -1.57916956e-02,  2.43755616e-02,\n        -8.82261712e-03, -5.90716898e-02, -1.48018533e-02,\n         7.57570891e-03, -4.07413463e-04, -1.75824761e-02,\n        -8.84100981e-03,  2.75795758e-02,  2.93627866e-02,\n        -3.39951999e-02, -4.92486246e-02,  6.61228271e-03,\n        -3.22939223e-03,  1.07017923e-02,  9.02211666e-02,\n        -2.41341181e-02,  7.26804975e-03, -6.17106184e-02,\n        -1.02135595e-02,  5.99205159e-02,  1.85735691e-02,\n        -1.66339893e-03,  1.60552338e-02, -1.85950752e-02,\n        -3.01274639e-02, -2.96964906e-02,  7.24639930e-03,\n         7.53716426e-03, -5.65933846e-02, -3.52073880e-03,\n        -4.21660542e-02, -3.19235623e-02, -4.15752679e-02,\n         4.65603173e-02, -3.52411605e-02,  7.04914331e-02,\n         2.82539427e-02,  2.62305886e-02, -2.68396139e-02,\n         5.35823544e-03,  2.42721662e-02, -3.31774615e-02,\n         1.71280112e-02,  1.00309821e-02,  5.96033372e-02,\n        -1.57134593e-01, -1.57632846e-02, -3.43542509e-02,\n        -2.04058085e-03,  3.21048312e-02, -7.85117745e-02,\n        -2.87321061e-02, -5.41485788e-04,  1.38391731e-02,\n         6.60932204e-03,  3.46348546e-02, -1.78076513e-02,\n         7.04169553e-03, -1.71841029e-02,  8.10983591e-03,\n        -3.51300091e-02,  8.10336694e-03,  3.36413980e-02,\n         1.17819374e-02, -6.39759377e-02, -1.60309393e-03,\n         8.72292835e-03,  1.54715090e-03, -3.83235700e-02,\n        -4.74458709e-02, -2.19518356e-02,  4.67976183e-02,\n         9.92611237e-03, -1.45563316e-02,  1.06840227e-02,\n        -4.77145333e-03, -2.53319889e-02,  3.77051602e-03,\n         1.59062743e-02,  1.49328252e-02,  9.13453288e-03,\n         1.54048298e-02, -1.51889706e-02,  5.38016260e-02,\n        -1.31512415e-02, -3.14215124e-02,  2.19399199e-01,\n        -2.57903636e-02,  3.64469667e-03,  1.11150499e-02,\n        -5.96752986e-02,  4.23414409e-02,  1.35953519e-02,\n         3.62979211e-02,  1.61038171e-02, -2.70835422e-02,\n         1.02463111e-01,  1.10725164e-02,  5.23587912e-02,\n        -3.04988120e-02, -3.24865170e-02, -2.31633428e-03,\n         3.73325348e-02, -5.98552786e-02,  1.43384226e-02,\n        -3.94654311e-02,  2.22652052e-02, -4.76025604e-03,\n         2.28501018e-02,  5.39685301e-02, -8.22809786e-02,\n        -8.80426168e-02, -2.86681261e-02, -1.71161629e-02,\n         2.97641587e-02, -3.16951200e-02]], dtype=float32)>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_pad = use_embedder(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T19:10:34.967234Z",
     "start_time": "2023-11-25T19:10:34.783346900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2214.054824677296\n"
     ]
    }
   ],
   "source": [
    "time_start = time.perf_counter()\n",
    "Xn_text_embed = [use_embedder(document) for document in dataset['block_documents']]\n",
    "time_end = time.perf_counter()\n",
    "print(time_end-time_start)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T18:43:25.953487600Z",
     "start_time": "2023-11-25T18:06:31.571382100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "padded_documents = []\n",
    "for document in Xn_text_embed:\n",
    "    pad = np.concatenate(np.repeat([use_pad], PARAGRAPH_QTY-document.shape[0], axis=0), axis=0)\n",
    "    padded_documents.append(tf.concat([document, pad], 0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T19:12:22.672771800Z",
     "start_time": "2023-11-25T19:12:19.952407200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "Xn_tensor = tf.convert_to_tensor(padded_documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T19:12:42.389113400Z",
     "start_time": "2023-11-25T19:12:41.572340400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "model_use = Sequential()\n",
    "model_use.add(Bidirectional(LSTM(512, return_sequences=True, input_shape=(PARAGRAPH_QTY, 512))))\n",
    "model_use.add(Bidirectional(LSTM(units=512, return_sequences=False)))\n",
    "model_use.add(Dense(256, activation='relu'))\n",
    "model_use.add(Dense(4, activation='softmax'))\n",
    "model_use.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"AUC\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T19:32:03.844800800Z",
     "start_time": "2023-11-25T19:32:03.843801Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 868s 868s/step - loss: 1.2739 - accuracy: 0.4628 - auc: 0.7252 - val_loss: 1.5278 - val_accuracy: 0.2453 - val_auc: 0.5884\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 866s 866s/step - loss: 1.5909 - accuracy: 0.2645 - auc: 0.5870 - val_loss: 1.2109 - val_accuracy: 0.2642 - val_auc: 0.6351\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 865s 865s/step - loss: 1.2387 - accuracy: 0.2479 - auc: 0.6307 - val_loss: 1.1960 - val_accuracy: 0.4906 - val_auc: 0.7561\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 865s 865s/step - loss: 1.2241 - accuracy: 0.4628 - auc: 0.7339 - val_loss: 1.1216 - val_accuracy: 0.4906 - val_auc: 0.7503\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 825s 825s/step - loss: 1.1710 - accuracy: 0.4628 - auc: 0.7401 - val_loss: 1.1032 - val_accuracy: 0.4906 - val_auc: 0.7429\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fc4fd7f0940>"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_use.fit(Xn_tensor, labels, validation_split=0.3, epochs=5, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T21:15:50.410350300Z",
     "start_time": "2023-11-25T20:04:21.399897800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_9 (Bidirectio  (None, 2048, 1024)       4198400   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirecti  (None, 1024)             6295552   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 1028      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,757,380\n",
      "Trainable params: 10,757,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_use.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T21:15:50.822681600Z",
     "start_time": "2023-11-25T21:15:50.395975800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clasificación de documentos con Longformer - Spanish"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/longformer-base-4096-spanish were not used when initializing RobertaModel: ['roberta.encoder.layer.6.attention.self.value_global.bias', 'roberta.encoder.layer.4.attention.self.value_global.weight', 'roberta.encoder.layer.1.attention.self.value_global.weight', 'roberta.encoder.layer.11.attention.self.value_global.bias', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.6.attention.self.value_global.weight', 'roberta.encoder.layer.0.attention.self.value_global.weight', 'roberta.encoder.layer.6.attention.self.query_global.bias', 'roberta.encoder.layer.10.attention.self.value_global.weight', 'roberta.encoder.layer.6.attention.self.key_global.weight', 'roberta.encoder.layer.7.attention.self.query_global.bias', 'roberta.encoder.layer.5.attention.self.value_global.weight', 'roberta.encoder.layer.8.attention.self.query_global.weight', 'roberta.encoder.layer.4.attention.self.query_global.weight', 'roberta.encoder.layer.7.attention.self.key_global.weight', 'roberta.encoder.layer.11.attention.self.value_global.weight', 'roberta.encoder.layer.1.attention.self.key_global.bias', 'roberta.encoder.layer.3.attention.self.value_global.weight', 'roberta.encoder.layer.11.attention.self.query_global.bias', 'roberta.encoder.layer.2.attention.self.query_global.weight', 'roberta.encoder.layer.8.attention.self.value_global.weight', 'roberta.encoder.layer.0.attention.self.value_global.bias', 'roberta.encoder.layer.3.attention.self.query_global.bias', 'roberta.encoder.layer.1.attention.self.key_global.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.3.attention.self.value_global.bias', 'roberta.encoder.layer.10.attention.self.query_global.bias', 'roberta.encoder.layer.5.attention.self.query_global.weight', 'roberta.encoder.layer.1.attention.self.query_global.bias', 'roberta.encoder.layer.5.attention.self.key_global.weight', 'roberta.encoder.layer.9.attention.self.value_global.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.4.attention.self.value_global.bias', 'roberta.encoder.layer.8.attention.self.value_global.bias', 'roberta.encoder.layer.4.attention.self.query_global.bias', 'roberta.encoder.layer.0.attention.self.key_global.bias', 'roberta.encoder.layer.0.attention.self.key_global.weight', 'roberta.encoder.layer.2.attention.self.key_global.weight', 'roberta.encoder.layer.9.attention.self.query_global.weight', 'roberta.encoder.layer.7.attention.self.value_global.weight', 'roberta.encoder.layer.5.attention.self.query_global.bias', 'roberta.encoder.layer.8.attention.self.key_global.bias', 'roberta.encoder.layer.10.attention.self.query_global.weight', 'roberta.encoder.layer.2.attention.self.value_global.weight', 'roberta.encoder.layer.8.attention.self.query_global.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.3.attention.self.key_global.bias', 'roberta.encoder.layer.2.attention.self.query_global.bias', 'roberta.encoder.layer.2.attention.self.key_global.bias', 'roberta.encoder.layer.3.attention.self.key_global.weight', 'roberta.encoder.layer.7.attention.self.key_global.bias', 'roberta.encoder.layer.0.attention.self.query_global.bias', 'roberta.encoder.layer.8.attention.self.key_global.weight', 'roberta.encoder.layer.9.attention.self.query_global.bias', 'roberta.encoder.layer.9.attention.self.key_global.bias', 'roberta.encoder.layer.3.attention.self.query_global.weight', 'roberta.encoder.layer.7.attention.self.query_global.weight', 'roberta.encoder.layer.11.attention.self.key_global.bias', 'roberta.encoder.layer.10.attention.self.key_global.weight', 'roberta.encoder.layer.6.attention.self.query_global.weight', 'roberta.encoder.layer.5.attention.self.key_global.bias', 'roberta.encoder.layer.10.attention.self.key_global.bias', 'roberta.encoder.layer.11.attention.self.key_global.weight', 'roberta.encoder.layer.9.attention.self.key_global.weight', 'roberta.encoder.layer.4.attention.self.key_global.weight', 'roberta.encoder.layer.4.attention.self.key_global.bias', 'roberta.encoder.layer.2.attention.self.value_global.bias', 'roberta.encoder.layer.0.attention.self.query_global.weight', 'roberta.encoder.layer.11.attention.self.query_global.weight', 'roberta.encoder.layer.6.attention.self.key_global.bias', 'roberta.encoder.layer.7.attention.self.value_global.bias', 'roberta.encoder.layer.1.attention.self.query_global.weight', 'roberta.encoder.layer.5.attention.self.value_global.bias', 'roberta.encoder.layer.10.attention.self.value_global.bias', 'roberta.encoder.layer.9.attention.self.value_global.bias', 'roberta.encoder.layer.1.attention.self.value_global.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at mrm8488/longformer-base-4096-spanish and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "long_tokenizer = RobertaTokenizer.from_pretrained(\"mrm8488/longformer-base-4096-spanish\",)\n",
    "long_model = RobertaModel.from_pretrained(\"mrm8488/longformer-base-4096-spanish\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T02:21:56.474241800Z",
     "start_time": "2023-11-26T02:21:36.343354700Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "from transformers import pipeline\n",
    "long_pipe = pipeline(model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "2023-11-25 23:19:42.304263: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-11-25 23:19:42.304362: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-11-25 23:19:42.304441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cn03): /proc/driver/nvidia/version does not exist\n",
      "2023-11-25 23:19:42.305121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inputs = long_tokenizer(\"\", return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = long_model(**inputs)\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "long_pad = tf.convert_to_tensor(cls_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T02:21:56.574171Z",
     "start_time": "2023-11-26T02:21:56.436215900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def long_embedder(document):\n",
    "    embeddings = []\n",
    "    for page in document:\n",
    "        inputs = long_tokenizer(page, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = long_model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings.append(cls_embeddings)\n",
    "    print(\"done document\")\n",
    "    return tf.convert_to_tensor(torch.cat(embeddings))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T04:11:54.541245Z",
     "start_time": "2023-11-26T04:11:54.474488300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n",
      "done document\n"
     ]
    }
   ],
   "source": [
    "Xn_long_embed = [long_embedder(document) for document in dataset['block_documents']]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-26T04:11:59.260070400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_long_documents = []\n",
    "for document in Xn_long_embed:\n",
    "    pad = np.concatenate(np.repeat([long_pad], PARAGRAPH_QTY-document.shape[0], axis=0), axis=0)\n",
    "    padded_long_documents.append(tf.concat([document, pad], 0))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Xn_long_tensor = tf.convert_to_tensor(padded_long_documents)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_long = Sequential()\n",
    "model_long.add(Bidirectional(LSTM(512, return_sequences=True, input_shape=(PARAGRAPH_QTY, 768))))\n",
    "model_long.add(Bidirectional(LSTM(units=512, return_sequences=False)))\n",
    "model_long.add(Dense(256, activation='relu'))\n",
    "model_long.add(Dense(4, activation='softmax'))\n",
    "model_long.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"AUC\"])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_long.fit(Xn_tensor, labels, validation_split=0.3, epochs=3, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_long.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clasificación de documentos con Tulio BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/tulio-chilean-spanish-bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/tulio-chilean-spanish-bert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tulio_tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/tulio-chilean-spanish-bert\")\n",
    "tulio_model = AutoModel.from_pretrained(\"dccuchile/tulio-chilean-spanish-bert\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T12:36:48.846312700Z",
     "start_time": "2023-11-26T12:36:46.238052200Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "from transformers import pipeline\n",
    "tulio_pipe = pipeline(\"fill-mask\", model=\"dccuchile/tulio-chilean-spanish-bert\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = tulio_tokenizer(\"\", return_tensors=\"pt\", padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = tulio_model(**inputs)\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "tulio_pad = tf.convert_to_tensor(cls_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tulio_embedder(document):\n",
    "    embeddings = []\n",
    "    for page in document:\n",
    "        inputs = tulio_tokenizer(page, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = tulio_model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings.append(cls_embeddings)\n",
    "    print(\"done document\")\n",
    "    return tf.convert_to_tensor(torch.cat(embeddings))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m Xn_tulio_embed \u001B[38;5;241m=\u001B[39m [tulio_embedder(document) \u001B[38;5;28;01mfor\u001B[39;00m document \u001B[38;5;129;01min\u001B[39;00m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mblock_documents\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n",
      "Cell \u001B[0;32mIn[33], line 1\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[0;32m----> 1\u001B[0m Xn_tulio_embed \u001B[38;5;241m=\u001B[39m [\u001B[43mtulio_embedder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocument\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m document \u001B[38;5;129;01min\u001B[39;00m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mblock_documents\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n",
      "Cell \u001B[0;32mIn[22], line 6\u001B[0m, in \u001B[0;36mtulio_embedder\u001B[0;34m(document)\u001B[0m\n\u001B[1;32m      4\u001B[0m inputs \u001B[38;5;241m=\u001B[39m tulio_tokenizer(page, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m----> 6\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtulio_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m cls_embeddings \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state[:, \u001B[38;5;241m0\u001B[39m, :]\n\u001B[1;32m      8\u001B[0m embeddings\u001B[38;5;241m.\u001B[39mappend(cls_embeddings)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1020\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1011\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m   1013\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m   1014\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m   1015\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1018\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m   1019\u001B[0m )\n\u001B[0;32m-> 1020\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1022\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1023\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1025\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1027\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1028\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1030\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1031\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1032\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1033\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:610\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    601\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    602\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    603\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    607\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    608\u001B[0m     )\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 610\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    612\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    615\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    616\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    618\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    620\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    621\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:495\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    483\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    484\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    485\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    492\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    493\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    494\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 495\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    502\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    504\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:425\u001B[0m, in \u001B[0;36mBertAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    416\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    417\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    423\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    424\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 425\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    426\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    427\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    428\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    429\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    430\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    431\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    433\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    434\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    435\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:323\u001B[0m, in \u001B[0;36mBertSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    320\u001B[0m     past_key_value \u001B[38;5;241m=\u001B[39m (key_layer, value_layer)\n\u001B[1;32m    322\u001B[0m \u001B[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001B[39;00m\n\u001B[0;32m--> 323\u001B[0m attention_scores \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_layer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelative_key\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelative_key_query\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    326\u001B[0m     query_length, key_length \u001B[38;5;241m=\u001B[39m query_layer\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], key_layer\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "Xn_tulio_embed = [tulio_embedder(document) for document in dataset['block_documents']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T12:36:58.773116300Z",
     "start_time": "2023-11-26T12:36:51.133611800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_tulio_documents = []\n",
    "for document in Xn_tulio_embed:\n",
    "    pad = np.concatenate(np.repeat([tulio_pad], PARAGRAPH_QTY-document.shape[0], axis=0), axis=0)\n",
    "    padded_long_documents.append(tf.concat([document, pad], 0))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Xn_tulio_tensor = tf.convert_to_tensor(padded_tulio_documents)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_tulio = Sequential()\n",
    "model_tulio.add(Bidirectional(LSTM(512, return_sequences=True, input_shape=(PARAGRAPH_QTY, 768))))\n",
    "model_tulio.add(Bidirectional(LSTM(units=512, return_sequences=False)))\n",
    "model_tulio.add(Dense(256, activation='relu'))\n",
    "model_tulio.add(Dense(4, activation='softmax'))\n",
    "model_tulio.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\", \"AUC\"])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_tulio.fit(Xn_tulio_tensor, labels, validation_split=0.3, epochs=3, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_tulio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmodel_tulio\u001B[49m\u001B[38;5;241m.\u001B[39msummary()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_tulio' is not defined"
     ]
    }
   ],
   "source": [
    "model_tulio.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T12:36:37.140539300Z",
     "start_time": "2023-11-26T12:36:37.072806300Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "PRUEBAS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "done tokenizing\n",
      "done encoding\n",
      "56.34367869887501\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "time_start = time.perf_counter()\n",
    "for page in dataset['block_documents'][0]:\n",
    "    inputs = tulio_tokenizer(page, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    print('done tokenizing')\n",
    "    with torch.no_grad():\n",
    "        outputs = tulio_model(**inputs)\n",
    "    print('done encoding')\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    embeddings.append(cls_embeddings)\n",
    "#Xn_long_text_embed = long_pipe(dataset['block_documents'][0])\n",
    "#Xn_long_text_embed = [long_pipe(document) for document in dataset['block_documents']]\n",
    "time_end = time.perf_counter()\n",
    "print(time_end-time_start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-26T03:41:56.511752Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done document\n"
     ]
    }
   ],
   "source": [
    "Xn_tulio_embed = [tulio_embedder(document) for document in dataset['block_documents']]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-26T03:45:03.238830900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "TensorShape([1415, 768])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xn_tensor = tf.convert_to_tensor(torch.cat(embeddings))\n",
    "Xn_tensor.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-26T00:37:17.888817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "padded_documents = []\n",
    "for document in embeddings:\n",
    "    pad = np.concatenate(np.repeat([long_pad], PARAGRAPH_QTY-document.shape[0], axis=0), axis=0)\n",
    "    padded_documents.append(tf.concat([document, pad], 0)) "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done tokenizing\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "EagerTensor object has no attribute 'size'. \n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()\n      ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m inputs \u001B[38;5;241m=\u001B[39m tulio_tokenizer(dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocuments\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m50\u001B[39m], return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdone tokenizing\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtulio_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdone encoding\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m cls_embeddings \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state[:, \u001B[38;5;241m0\u001B[39m, :]\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:968\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    966\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    967\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 968\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m \u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m()\n\u001B[1;32m    969\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    970\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m inputs_embeds\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:438\u001B[0m, in \u001B[0;36mTensor.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name):\n\u001B[1;32m    435\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mT\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mastype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mravel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranspose\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreshape\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclip\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msize\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    436\u001B[0m               \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtolist\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m}:\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001B[39;00m\n\u001B[0;32m--> 438\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m    439\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m    440\u001B[0m \u001B[38;5;124m      If you are looking for numpy-related methods, please run the following:\u001B[39m\n\u001B[1;32m    441\u001B[0m \u001B[38;5;124m      from tensorflow.python.ops.numpy_ops import np_config\u001B[39m\n\u001B[1;32m    442\u001B[0m \u001B[38;5;124m      np_config.enable_numpy_behavior()\u001B[39m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;124m    \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[1;32m    444\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(name)\n",
      "\u001B[0;31mAttributeError\u001B[0m: EagerTensor object has no attribute 'size'. \n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()\n      "
     ]
    }
   ],
   "source": [
    "time_start = time.perf_counter()\n",
    "inputs = tulio_tokenizer(dataset['documents'][50], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print('done tokenizing')\n",
    "outputs = tulio_model(**inputs)\n",
    "print('done encoding')\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "#Xn_long_text_embed = long_pipe(dataset['block_documents'][0])\n",
    "#Xn_long_text_embed = [long_pipe(document) for document in dataset['block_documents']]\n",
    "time_end = time.perf_counter()\n",
    "print(time_end-time_start)\n",
    "cls_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-26T00:05:03.579088300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 768])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T12:36:30.005330200Z",
     "start_time": "2023-11-26T12:36:29.953403100Z"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "time_start = time.perf_counter()\n",
    "inputs = tulio_tokenizer(dataset['block_documents'][0], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print('done tokenizing')\n",
    "outputs = tulio_model(**inputs)\n",
    "print('done encoding')\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "#Xn_long_text_embed = long_pipe(dataset['block_documents'][0])\n",
    "#Xn_long_text_embed = [long_pipe(document) for document in dataset['block_documents']]\n",
    "time_end = time.perf_counter()\n",
    "print(time_end-time_start)\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
